{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Fake Review Detection 03182023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Combine review content with labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) loading reviewContent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_review=\"D:/Group Project/YelpZip/reviewContent.txt\"\n",
    "review=pd.read_csv(file_review,sep=\"\\t\",header=None)\n",
    "review.columns=['user_id', 'prod_id', 'date', 'review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  prod_id        date  \\\n",
      "0     5044        0  2014-11-16   \n",
      "1     5045        0  2014-09-08   \n",
      "2     5046        0  2013-10-06   \n",
      "3     5047        0  2014-11-30   \n",
      "4     5048        0  2014-08-28   \n",
      "\n",
      "                                              review  \n",
      "0  Drinks were bad, the hot chocolate was watered...  \n",
      "1  This was the worst experience I've ever had a ...  \n",
      "2  This is located on the site of the old Spruce ...  \n",
      "3  I enjoyed coffee and breakfast twice at Toast ...  \n",
      "4  I love Toast! The food choices are fantastic -...  \n",
      "number of rows of reviewContent: 608458\n"
     ]
    }
   ],
   "source": [
    "print(review.head())\n",
    "print(\"number of rows of reviewContent:\",len(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For some reason, the Notepad++ reads 608598 rows from reviewContent, but the python only reads 608458 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) loading metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_meta=\"D:/Group Project/YelpZip/metadata.txt\"\n",
    "meta=pd.read_csv(file_meta,sep=\"\\t\",header=None)\n",
    "meta.columns=[\"user_id\",\"prod_id\",\"rating\",\"label\",\"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  prod_id  rating  label        date\n",
      "0     5044        0     1.0     -1  2014-11-16\n",
      "1     5045        0     1.0     -1  2014-09-08\n",
      "2     5046        0     3.0     -1  2013-10-06\n",
      "3     5047        0     5.0     -1  2014-11-30\n",
      "4     5048        0     5.0     -1  2014-08-28\n",
      "number of rows of meta: 608598\n"
     ]
    }
   ],
   "source": [
    "print(meta.head())\n",
    "print(\"number of rows of meta:\",len(meta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) combine the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  prod_id        date  \\\n",
      "0     5044        0  2014-11-16   \n",
      "1     5045        0  2014-09-08   \n",
      "2     5046        0  2013-10-06   \n",
      "3     5047        0  2014-11-30   \n",
      "4     5048        0  2014-08-28   \n",
      "\n",
      "                                              review  rating  label  \n",
      "0  Drinks were bad, the hot chocolate was watered...     1.0     -1  \n",
      "1  This was the worst experience I've ever had a ...     1.0     -1  \n",
      "2  This is located on the site of the old Spruce ...     3.0     -1  \n",
      "3  I enjoyed coffee and breakfast twice at Toast ...     5.0     -1  \n",
      "4  I love Toast! The food choices are fantastic -...     5.0     -1  \n",
      "length of join_data: 608458\n"
     ]
    }
   ],
   "source": [
    "join_data=review.merge(meta,on=[\"user_id\",\"date\",\"prod_id\"],how=\"left\")\n",
    "print(join_data.head())\n",
    "print(\"length of join_data:\",len(join_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The length of combined data is 608458."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) export combined data to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import savetxt\n",
    "savetxt(\"orig_review_with_labeling.txt\",join_data,fmt=\"%s\",delimiter=\"\\t\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining the \"orig_review_with_labeling.txt\" with metadata again can generate dataset with 608598 rows without missing samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  prod_id        date  \\\n",
      "0     5044        0  2014-11-16   \n",
      "1     5045        0  2014-09-08   \n",
      "2     5046        0  2013-10-06   \n",
      "3     5047        0  2014-11-30   \n",
      "4     5048        0  2014-08-28   \n",
      "\n",
      "                                              review  rating  label  \n",
      "0  Drinks were bad, the hot chocolate was watered...     1.0   -1.0  \n",
      "1  This was the worst experience I've ever had a ...     1.0   -1.0  \n",
      "2  This is located on the site of the old Spruce ...     3.0   -1.0  \n",
      "3  I enjoyed coffee and breakfast twice at Toast ...     5.0   -1.0  \n",
      "4  I love Toast! The food choices are fantastic -...     5.0   -1.0  \n",
      "number of rows of reviewContent: 608598\n",
      "check the null data: True\n"
     ]
    }
   ],
   "source": [
    "file_review2=\"C:/Users/Lu/PycharmProjects/Group_Project/orig_review_with_labeling.txt\"\n",
    "review2=pd.read_csv(file_review2,sep=\"\\t\",header=None)\n",
    "review2.columns=['user_id', 'prod_id', 'date', 'review', 'rating', 'label']\n",
    "print(review2.head())\n",
    "print(\"number of rows of reviewContent:\",len(review2))\n",
    "print(\"check the null data:\",review2.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Though 608598 rows are read by python from the combined data this time, there are missing values. So the two datasets are combined again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  prod_id  rating  label        date\n",
      "0     5044        0     1.0     -1  2014-11-16\n",
      "1     5045        0     1.0     -1  2014-09-08\n",
      "2     5046        0     3.0     -1  2013-10-06\n",
      "3     5047        0     5.0     -1  2014-11-30\n",
      "4     5048        0     5.0     -1  2014-08-28\n",
      "number of rows of meta: 608598\n"
     ]
    }
   ],
   "source": [
    "file_meta2=\"D:/Group Project/YelpZip/metadata.txt\"\n",
    "meta2=pd.read_csv(file_meta2,sep=\"\\t\",header=None)\n",
    "meta2.columns=[\"user_id\",\"prod_id\",\"rating\",\"label\",\"date\"]\n",
    "print(meta2.head())\n",
    "print(\"number of rows of meta:\",len(meta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  prod_id        date  \\\n",
      "0     5044        0  2014-11-16   \n",
      "1     5045        0  2014-09-08   \n",
      "2     5046        0  2013-10-06   \n",
      "3     5047        0  2014-11-30   \n",
      "4     5048        0  2014-08-28   \n",
      "\n",
      "                                              review  rating  label  \n",
      "0  Drinks were bad, the hot chocolate was watered...     1.0     -1  \n",
      "1  This was the worst experience I've ever had a ...     1.0     -1  \n",
      "2  This is located on the site of the old Spruce ...     3.0     -1  \n",
      "3  I enjoyed coffee and breakfast twice at Toast ...     5.0     -1  \n",
      "4  I love Toast! The food choices are fantastic -...     5.0     -1  \n",
      "length of join_data: 608598\n",
      "check the null data: False\n"
     ]
    }
   ],
   "source": [
    "review2=review2.drop([\"label\",\"rating\"],axis=1)\n",
    "join_data2=review2.merge(meta2,on=[\"user_id\",\"date\",\"prod_id\"],how=\"left\")\n",
    "print(join_data2.head())\n",
    "print(\"length of join_data:\",len(join_data2))\n",
    "print(\"check the null data:\",join_data2.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "savetxt(\"orig_review_with_labeling_608598rows.txt\",join_data2,fmt=\"%s\",delimiter=\"\\t\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing of the review content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  prod_id        date  \\\n",
      "0     5044        0  2014-11-16   \n",
      "1     5045        0  2014-09-08   \n",
      "2     5046        0  2013-10-06   \n",
      "3     5047        0  2014-11-30   \n",
      "4     5048        0  2014-08-28   \n",
      "\n",
      "                                              review  rating  label  \n",
      "0  Drinks were bad, the hot chocolate was watered...     1.0     -1  \n",
      "1  This was the worst experience I've ever had a ...     1.0     -1  \n",
      "2  This is located on the site of the old Spruce ...     3.0     -1  \n",
      "3  I enjoyed coffee and breakfast twice at Toast ...     5.0     -1  \n",
      "4  I love Toast! The food choices are fantastic -...     5.0     -1  \n",
      "length of data: 608598\n",
      "check the null data: False\n"
     ]
    }
   ],
   "source": [
    "file_review3=\"C:/Users/Lu/PycharmProjects/Group_Project/orig_review_with_labeling_608598rows.txt\"\n",
    "review3=pd.read_csv(file_review3,sep=\"\\t\",header=None)\n",
    "review3.columns=['user_id', 'prod_id', 'date', 'review', 'rating', 'label']\n",
    "print(review3.head())\n",
    "print(\"length of data:\",len(review3))\n",
    "print(\"check the null data:\",review3.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "data=review3.copy()\n",
    "print(len(data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(608598, 7)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "length=len(data)\n",
    "empty_col=np.empty([length,1])\n",
    "data=np.append(data,empty_col,1)\n",
    "print(data.shape)\n",
    "for i in range(length):\n",
    "    # a.lower the text\n",
    "    text=data[i,3].lower()\n",
    "    \n",
    "    # b.remove white spaces if there's any        No difference after tokenization, so this step can be removed\n",
    "    # text = text.strip()\n",
    "    \n",
    "    # b.contraction words\n",
    "    text = text.replace(\"_\", \" \")\n",
    "    text = re.sub(br'(\\xc2)(.)', b'', text.encode('utf-8')).decode()\n",
    "    text = text.replace(\"can't\", \"can not\")\n",
    "    text = text.replace(\"won't\", \"will not\")\n",
    "    text = text.replace(\"'ve\",\" have\")\n",
    "    text = text.replace(\"'d\",\" had\")\n",
    "    text = text.replace(\"'m\", \" am\")\n",
    "    text = text.replace(\"'ll\", \" will\")\n",
    "    text = text.replace(\"'s\", \" is\")\n",
    "    text = text.replace(\"n't\", \" not\")\n",
    "    text = text.replace(\"'re\", \" are\")\n",
    "    text = text.replace(\"st.\", \"street\")\n",
    "    text = text.replace(\"bldg.\", \"building\") \n",
    "    \n",
    "    # c.deal with punctuation such as ‘!”#$%&'()*+,-./:;?@[\\]^_`{|}~’, and including \"...\",\"???\"...\n",
    "    text=re.sub(r\"[^\\w\\s]\", \" \", text) \n",
    "\n",
    "#     # d.remove punctuation marks such as ‘!”#$%&'()*+,-./:;?@[\\]^_`{|}~’\n",
    "#     text = \"\".join([i for i in text if i not in string.punctuation])\n",
    "\n",
    "    # d.remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # e.tokenization\n",
    "    word_tokens = word_tokenize(text)  #this is a list\n",
    "\n",
    "    # f.remove stopwords\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    text=[w for w in word_tokens if w not in stop_words]\n",
    "\n",
    "    # g.lemmatization\n",
    "    text = [WordNetLemmatizer().lemmatize(word,\"a\") for word in text]\n",
    "    text = [WordNetLemmatizer().lemmatize(word, \"v\") for word in text]\n",
    "    text = [WordNetLemmatizer().lemmatize(word, \"n\") for word in text]\n",
    "    text = [WordNetLemmatizer().lemmatize(word, \"s\") for word in text]\n",
    "    text = [WordNetLemmatizer().lemmatize(word, \"r\") for word in text]\n",
    "\n",
    "    data[i,6]=text\n",
    "#print(data[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It takes a while to run the preprocessing of every text content, so the output was saved to txt file for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "savetxt(\"orig_review_with_labeling_608598rows_af_lemma.txt\",data,fmt=\"%s\",delimiter=\"\\t\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  prod_id        date  \\\n",
      "0     5044        0  2014-11-16   \n",
      "1     5045        0  2014-09-08   \n",
      "2     5046        0  2013-10-06   \n",
      "3     5047        0  2014-11-30   \n",
      "4     5048        0  2014-08-28   \n",
      "\n",
      "                                              review  rating  label  \\\n",
      "0  Drinks were bad, the hot chocolate was watered...     1.0     -1   \n",
      "1  This was the worst experience I've ever had a ...     1.0     -1   \n",
      "2  This is located on the site of the old Spruce ...     3.0     -1   \n",
      "3  I enjoyed coffee and breakfast twice at Toast ...     5.0     -1   \n",
      "4  I love Toast! The food choices are fantastic -...     5.0     -1   \n",
      "\n",
      "                                               lemma  \n",
      "0  ['drink', 'bad', 'hot', 'chocolate', 'water', ...  \n",
      "1  ['bad', 'experience', 'ever', 'casual', 'coffe...  \n",
      "2  ['locate', 'site', 'old', 'spruce', 'street', ...  \n",
      "3  ['enjoy', 'coffee', 'breakfast', 'twice', 'toa...  \n",
      "4  ['love', 'toast', 'food', 'choice', 'fantastic...  \n",
      "length of data: 608598\n",
      "check the null data: False\n"
     ]
    }
   ],
   "source": [
    "file_review4=\"D:/Lu Yu/orig_review_with_labeling_608598rows_af_lemma.txt\"\n",
    "review4=pd.read_csv(file_review4,sep=\"\\t\",header=None)\n",
    "review4.columns=['user_id', 'prod_id', 'date', 'review', 'rating', 'label','lemma']\n",
    "print(review4.head())\n",
    "print(\"length of data:\",len(review4))\n",
    "print(\"check the null data:\",review4.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if some words are still in the word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 79244\n"
     ]
    }
   ],
   "source": [
    "lemma_test=review4[['lemma']]\n",
    "count=0\n",
    "for i in range(len(review4)):\n",
    "    if 'drink' in lemma_test.iloc[i][0]:\n",
    "        #print(\"i:\",i)\n",
    "        count+=1\n",
    "print(\"count:\",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 0\n"
     ]
    }
   ],
   "source": [
    "lemma_test=review4[['lemma']]\n",
    "count=0\n",
    "for i in range(len(review4)):\n",
    "    if 'xadinterest' in lemma_test.iloc[i][0]:\n",
    "        print(\"i:\",i)\n",
    "        count+=1\n",
    "print(\"count:\",count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The words start with x, such as xad has already been removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['woww',\n",
       " 'wowwed',\n",
       " 'wowwee',\n",
       " 'wowwer',\n",
       " 'wowwie',\n",
       " 'wowwing',\n",
       " 'wowwowow',\n",
       " 'wowww',\n",
       " 'wowwww',\n",
       " 'wowwwww',\n",
       " 'wowwwwwed',\n",
       " 'wowwwwweee',\n",
       " 'wowwwwww',\n",
       " 'wowwwwwww',\n",
       " 'wowwwwwwww',\n",
       " 'wowwwwwwwww',\n",
       " 'wowwwwwwwwwwww',\n",
       " 'wowwy',\n",
       " 'wowy',\n",
       " 'wowza',\n",
       " 'wowzaaa',\n",
       " 'wowzah',\n",
       " 'wowzas',\n",
       " 'wowzer',\n",
       " 'wowzers',\n",
       " 'wowzerz',\n",
       " 'wowzie',\n",
       " 'wowzzz',\n",
       " 'wp',\n",
       " 'wpa',\n",
       " 'wpb',\n",
       " 'wpe',\n",
       " 'wph',\n",
       " 'wphilly',\n",
       " 'wpicy',\n",
       " 'wplenty',\n",
       " 'wps',\n",
       " 'wpstreetcom',\n",
       " 'wpu',\n",
       " 'wpunj',\n",
       " 'wpxo',\n",
       " 'wq',\n",
       " 'wqas',\n",
       " 'wqs',\n",
       " 'wqwqdesert',\n",
       " 'wr',\n",
       " 'wra',\n",
       " 'wrack',\n",
       " 'wrackedly',\n",
       " 'wram',\n",
       " 'wrangle',\n",
       " 'wrangler',\n",
       " 'wrap',\n",
       " 'wraparound',\n",
       " 'wrapepd',\n",
       " 'wrapido',\n",
       " 'wrappd',\n",
       " 'wrappe',\n",
       " 'wrappede',\n",
       " 'wrappen',\n",
       " 'wrapper',\n",
       " 'wrapping',\n",
       " 'wrappper',\n",
       " 'wrassle',\n",
       " 'wrassler',\n",
       " 'wrasslers',\n",
       " 'wrath',\n",
       " 'wray',\n",
       " 'wrd',\n",
       " 'wre',\n",
       " 'wreak',\n",
       " 'wreath',\n",
       " 'wreathe',\n",
       " 'wreck',\n",
       " 'wreckage',\n",
       " 'wreckbass',\n",
       " 'wrecker',\n",
       " 'wreckfish',\n",
       " 'wreckless',\n",
       " 'wreckroom',\n",
       " 'wree',\n",
       " 'wren',\n",
       " 'wrench',\n",
       " 'wrenchingly',\n",
       " 'wrere',\n",
       " 'wresle',\n",
       " 'wrest',\n",
       " 'wrestle',\n",
       " 'wrestlemania',\n",
       " 'wrestler',\n",
       " 'wretch',\n",
       " 'wretched',\n",
       " 'wretchedly',\n",
       " 'wretching',\n",
       " 'wretchingly',\n",
       " 'wribs',\n",
       " 'wriet',\n",
       " 'wriggle',\n",
       " 'wright',\n",
       " 'wrighteous',\n",
       " 'wrighthouse',\n",
       " 'wrigley',\n",
       " 'wriiting',\n",
       " 'wriitng',\n",
       " 'wring',\n",
       " 'wringer',\n",
       " 'wrinkle',\n",
       " 'wrinkly',\n",
       " 'wrist',\n",
       " 'wristband',\n",
       " 'wristbanded',\n",
       " 'wristed',\n",
       " 'wristlet',\n",
       " 'wristreet',\n",
       " 'wristwatch',\n",
       " 'writ',\n",
       " 'write',\n",
       " 'writeareview',\n",
       " 'writen',\n",
       " 'writer',\n",
       " 'writeup',\n",
       " 'writeups',\n",
       " 'writhe',\n",
       " 'writig',\n",
       " 'writingnonnthe',\n",
       " 'writng',\n",
       " 'writtin',\n",
       " 'writting',\n",
       " 'wrj',\n",
       " 'wrk',\n",
       " 'wrll',\n",
       " 'wrming',\n",
       " 'wroing',\n",
       " 'wrold',\n",
       " 'wromg',\n",
       " 'wrond',\n",
       " 'wrong',\n",
       " 'wrongdoing',\n",
       " 'wrongfully',\n",
       " 'wrongg',\n",
       " 'wronggg',\n",
       " 'wrongggg',\n",
       " 'wrongheaded',\n",
       " 'wrongly',\n",
       " 'wrongness',\n",
       " 'wrongo',\n",
       " 'wrongt',\n",
       " 'wrongwrong',\n",
       " 'wront',\n",
       " 'wronv',\n",
       " 'wrooong',\n",
       " 'wrooonggg',\n",
       " 'wroooong',\n",
       " 'wrooooong',\n",
       " 'wrost',\n",
       " 'wroth',\n",
       " 'wrrroonngg',\n",
       " 'wrt',\n",
       " 'wrth',\n",
       " 'wrtiten',\n",
       " 'wrx',\n",
       " 'wry',\n",
       " 'wryly',\n",
       " 'wsa',\n",
       " 'wsburg',\n",
       " 'wsh',\n",
       " 'wshing',\n",
       " 'wsit',\n",
       " 'wsiter',\n",
       " 'wsiting',\n",
       " 'wsj',\n",
       " 'wsl',\n",
       " 'wsm',\n",
       " 'wso',\n",
       " 'wsp',\n",
       " 'wsq',\n",
       " 'wst',\n",
       " 'wstaff',\n",
       " 'wsu',\n",
       " 'wsw',\n",
       " 'wt',\n",
       " 'wtc',\n",
       " 'wtcdss',\n",
       " 'wtermelon',\n",
       " 'wtever',\n",
       " 'wtf',\n",
       " 'wtfery',\n",
       " 'wtff',\n",
       " 'wtfff',\n",
       " 'wtfism',\n",
       " 'wtflippers',\n",
       " 'wtforeo',\n",
       " 'wtfs',\n",
       " 'wtfuck',\n",
       " 'wtg',\n",
       " 'wth',\n",
       " 'wthe',\n",
       " 'wtheck',\n",
       " 'wthell',\n",
       " 'wthh',\n",
       " 'wthin',\n",
       " 'wthout',\n",
       " 'wtic',\n",
       " 'wtih',\n",
       " 'wtihin',\n",
       " 'wtihout',\n",
       " 'wtith',\n",
       " 'wtkwfedq',\n",
       " 'wtnh',\n",
       " 'wto',\n",
       " 'wtr',\n",
       " 'wts',\n",
       " 'wtv',\n",
       " 'wu',\n",
       " 'wuality',\n",
       " 'wuan',\n",
       " 'wub',\n",
       " 'wud',\n",
       " 'wuda',\n",
       " 'wudda',\n",
       " 'wude',\n",
       " 'wudnt',\n",
       " 'wudve',\n",
       " 'wuh',\n",
       " 'wuickly',\n",
       " 'wuilln',\n",
       " 'wuin',\n",
       " 'wuld',\n",
       " 'wulfgang',\n",
       " 'wull',\n",
       " 'wullah',\n",
       " 'wulled',\n",
       " 'wun',\n",
       " 'wunder',\n",
       " 'wunderbar',\n",
       " 'wundergroundmusic',\n",
       " 'wundergun',\n",
       " 'wunderkind',\n",
       " 'wunderkinds',\n",
       " 'wunderwurst',\n",
       " 'wunna',\n",
       " 'wunschkonzert',\n",
       " 'wuntong',\n",
       " 'wuntun',\n",
       " 'wunwun',\n",
       " 'wuold',\n",
       " 'wuornos',\n",
       " 'wup',\n",
       " 'wurde',\n",
       " 'wurden',\n",
       " 'wurst',\n",
       " 'wurste',\n",
       " 'wurstel',\n",
       " 'wursthaus',\n",
       " 'wurstkuche',\n",
       " 'wurstküche',\n",
       " 'wurstplates',\n",
       " 'wurstplatte',\n",
       " 'wurstplatten',\n",
       " 'wurstreet',\n",
       " 'wursts',\n",
       " 'wurstsalad',\n",
       " 'wurstz',\n",
       " 'wurts',\n",
       " 'wurtsel',\n",
       " 'wurtz',\n",
       " 'wurzhaus',\n",
       " 'wuss',\n",
       " 'wussed',\n",
       " 'wussie',\n",
       " 'wussiest',\n",
       " 'wussification',\n",
       " 'wussified',\n",
       " 'wussing',\n",
       " 'wussten',\n",
       " 'wussy',\n",
       " 'wuster',\n",
       " 'wut',\n",
       " 'wutang',\n",
       " 'wutch',\n",
       " 'wutevas',\n",
       " 'wuth',\n",
       " 'wuthering',\n",
       " 'wuthin',\n",
       " 'wuts',\n",
       " 'wuuuh',\n",
       " 'wuuuut',\n",
       " 'wuuuuttttt',\n",
       " 'wuv',\n",
       " 'wuxi',\n",
       " 'wuz',\n",
       " 'wuzzup',\n",
       " 'wuzzy',\n",
       " 'wv',\n",
       " 'wve',\n",
       " 'wven',\n",
       " 'wver',\n",
       " 'wvil',\n",
       " 'wvill',\n",
       " 'wvillage',\n",
       " 'wvu',\n",
       " 'ww',\n",
       " 'wwaaayyy',\n",
       " 'wwaaayyyy',\n",
       " 'wwaayy',\n",
       " 'wwanted',\n",
       " 'wwas',\n",
       " 'wwayyyyy',\n",
       " 'wwd',\n",
       " 'wwe',\n",
       " 'wwere',\n",
       " 'wwf',\n",
       " 'wwhhhaaaa',\n",
       " 'wwi',\n",
       " 'wwii',\n",
       " 'wwith',\n",
       " 'wwje',\n",
       " 'wwjkd',\n",
       " 'wwork',\n",
       " 'wwp',\n",
       " 'wws',\n",
       " 'wwtm',\n",
       " 'www',\n",
       " 'wwwaaayyyy',\n",
       " 'wwwlinkedin',\n",
       " 'wwwuuude',\n",
       " 'wwww',\n",
       " 'wwwwaaaayyyy',\n",
       " 'wwwwaaayyyyy',\n",
       " 'wwwwooooowwww',\n",
       " 'wx',\n",
       " 'wxcept',\n",
       " 'wxou',\n",
       " 'wxp',\n",
       " 'wxpn',\n",
       " 'wy',\n",
       " 'wyagu',\n",
       " 'wyatt',\n",
       " 'wych',\n",
       " 'wychas',\n",
       " 'wyck',\n",
       " 'wyckoff',\n",
       " 'wycliffe',\n",
       " 'wycoff',\n",
       " 'wye',\n",
       " 'wyeth',\n",
       " 'wygu',\n",
       " 'wyhdam',\n",
       " 'wykno',\n",
       " 'wyland',\n",
       " 'wylie',\n",
       " 'wyndham',\n",
       " 'wyndmoor',\n",
       " 'wyne',\n",
       " 'wynewood',\n",
       " 'wynn',\n",
       " 'wynne',\n",
       " 'wynnewood',\n",
       " 'wynns',\n",
       " 'wynnwood',\n",
       " 'wyoming',\n",
       " 'wysin',\n",
       " 'wysiwyg',\n",
       " 'wythe',\n",
       " 'wzn',\n",
       " 'wzns',\n",
       " 'wzyeae',\n",
       " 'wählen',\n",
       " 'wären',\n",
       " 'wölffer',\n",
       " 'wöw',\n",
       " 'wünsche',\n",
       " 'würde',\n",
       " 'würden',\n",
       " 'würst',\n",
       " 'würste',\n",
       " 'würstel',\n",
       " 'würsts',\n",
       " 'wōden',\n",
       " 'wōdnesdæg',\n",
       " 'xa',\n",
       " 'xacuti',\n",
       " 'xacutti',\n",
       " 'xai',\n",
       " 'xaio',\n",
       " 'xan',\n",
       " 'xana',\n",
       " 'xanadu',\n",
       " 'xanax',\n",
       " 'xanex',\n",
       " 'xanga',\n",
       " 'xangas',\n",
       " 'xango',\n",
       " 'xangos',\n",
       " 'xangurro',\n",
       " 'xanny',\n",
       " 'xantham',\n",
       " 'xanthan',\n",
       " 'xanthum',\n",
       " 'xao',\n",
       " 'xarel',\n",
       " 'xavi',\n",
       " 'xavier',\n",
       " 'xbanana',\n",
       " 'xbetter',\n",
       " 'xbh',\n",
       " 'xbl',\n",
       " 'xbls',\n",
       " 'xblt',\n",
       " 'xbox',\n",
       " 'xc',\n",
       " 'xcelerator',\n",
       " 'xcellend',\n",
       " 'xcellent',\n",
       " 'xcelorator',\n",
       " 'xcept',\n",
       " 'xceptional',\n",
       " 'xclnt',\n",
       " 'xcwe',\n",
       " 'xcz',\n",
       " 'xd',\n",
       " 'xdd',\n",
       " 'xdnnow',\n",
       " 'xe',\n",
       " 'xed',\n",
       " 'xellent',\n",
       " 'xena',\n",
       " 'xeni',\n",
       " 'xenia',\n",
       " 'xenn',\n",
       " 'xeno',\n",
       " 'xenophobe',\n",
       " 'xenophobes',\n",
       " 'xenophobia',\n",
       " 'xenophobic',\n",
       " 'xeo',\n",
       " 'xeoh',\n",
       " 'xeres',\n",
       " 'xerox',\n",
       " 'xers',\n",
       " 'xeu',\n",
       " 'xeulu',\n",
       " 'xfinity',\n",
       " 'xh',\n",
       " 'xhange',\n",
       " 'xhpregzqbvy',\n",
       " 'xi',\n",
       " 'xia',\n",
       " 'xiagu',\n",
       " 'xiahe',\n",
       " 'xialongbao',\n",
       " 'xialongbaos',\n",
       " 'xialongboa',\n",
       " 'xiam',\n",
       " 'xiamen',\n",
       " 'xian',\n",
       " 'xiandu',\n",
       " 'xiang',\n",
       " 'xiangguo',\n",
       " 'xianlongbao',\n",
       " 'xiao',\n",
       " 'xiaoloanbaos',\n",
       " 'xiaolonbao',\n",
       " 'xiaolong',\n",
       " 'xiaolongbai',\n",
       " 'xiaolongbao',\n",
       " 'xiaolongbaos',\n",
       " 'xiaolongmantou',\n",
       " 'xiaolungbaos',\n",
       " 'xiaomian',\n",
       " 'xie',\n",
       " 'xiet',\n",
       " 'xiexie',\n",
       " 'xii',\n",
       " 'xil',\n",
       " 'xim',\n",
       " 'ximenez',\n",
       " 'ximénez',\n",
       " 'xin',\n",
       " 'xinches',\n",
       " 'xing',\n",
       " 'xingu',\n",
       " 'xinh',\n",
       " 'xinjiang',\n",
       " 'xinomavro',\n",
       " 'xinouli',\n",
       " 'xinxiang',\n",
       " 'xio',\n",
       " 'xiolongbao',\n",
       " 'xiomara',\n",
       " 'xios',\n",
       " 'xiou',\n",
       " 'xipa',\n",
       " 'xipirones',\n",
       " 'xisa',\n",
       " 'xish',\n",
       " 'xiu',\n",
       " 'xiulong',\n",
       " 'xiumai',\n",
       " 'xiv',\n",
       " 'xix',\n",
       " 'xixa',\n",
       " 'xixappeal',\n",
       " 'xjvg',\n",
       " 'xkcpzcqa',\n",
       " 'xl',\n",
       " 'xlarge',\n",
       " 'xlb',\n",
       " 'xlbs',\n",
       " 'xlerator',\n",
       " 'xlg',\n",
       " 'xlh',\n",
       " 'xlnt',\n",
       " 'xltb',\n",
       " 'xm',\n",
       " 'xmas',\n",
       " 'xmbke',\n",
       " 'xme',\n",
       " 'xmslhvpg',\n",
       " 'xnayed',\n",
       " 'xne',\n",
       " 'xnext',\n",
       " 'xny',\n",
       " 'xo',\n",
       " 'xochil',\n",
       " 'xochilmilco',\n",
       " 'xochiltl',\n",
       " 'xochitl',\n",
       " 'xochitli',\n",
       " 'xochitlphilly',\n",
       " 'xochitls',\n",
       " 'xochtil',\n",
       " 'xoco',\n",
       " 'xocotal',\n",
       " 'xodn',\n",
       " 'xoi',\n",
       " 'xokq',\n",
       " 'xolé',\n",
       " 'xooler',\n",
       " 'xooxox',\n",
       " 'xooxoxoxoxox',\n",
       " 'xorta',\n",
       " 'xot',\n",
       " 'xotchil',\n",
       " 'xotchitl',\n",
       " 'xox',\n",
       " 'xoxo',\n",
       " 'xoxox',\n",
       " 'xoxoxo',\n",
       " 'xoxoxoooo',\n",
       " 'xoxoxox',\n",
       " 'xoxoxoxo',\n",
       " 'xoxoxoxoxo',\n",
       " 'xoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxox',\n",
       " 'xoz',\n",
       " 'xp',\n",
       " 'xperience',\n",
       " 'xpl',\n",
       " 'xpm',\n",
       " 'xpn',\n",
       " 'xqbl',\n",
       " 'xqka',\n",
       " 'xquisite',\n",
       " 'xrap',\n",
       " 'xrated',\n",
       " 'xray',\n",
       " 'xrkhg',\n",
       " 'xrn',\n",
       " 'xsports',\n",
       " 'xspzfpvi',\n",
       " 'xstar',\n",
       " 'xtakeaway',\n",
       " 'xtian',\n",
       " 'xtra',\n",
       " 'xtreme',\n",
       " 'xtremely',\n",
       " 'xtyne',\n",
       " 'xu',\n",
       " 'xua',\n",
       " 'xuan',\n",
       " 'xuaolong',\n",
       " 'xue',\n",
       " 'xuecai',\n",
       " 'xueyu',\n",
       " 'xui',\n",
       " 'xun',\n",
       " 'xunta',\n",
       " 'xuntas',\n",
       " 'xuong',\n",
       " 'xuyen',\n",
       " 'xuyuen',\n",
       " 'xuyén',\n",
       " 'xuyên',\n",
       " 'xvartha',\n",
       " 'xwk',\n",
       " 'xx',\n",
       " 'xxk',\n",
       " 'xxl',\n",
       " 'xxlg',\n",
       " 'xxo',\n",
       " 'xxoo',\n",
       " 'xxsilog',\n",
       " 'xxsilogs',\n",
       " 'xxx',\n",
       " 'xxxl',\n",
       " 'xxxlovedxxx',\n",
       " 'xxxtra',\n",
       " 'xxxx',\n",
       " 'xxxxing',\n",
       " 'xxxxth',\n",
       " 'xxxxx',\n",
       " 'xxxxxx',\n",
       " 'xxxxxxtra',\n",
       " 'xxxxxxxxxx',\n",
       " 'xxxxxxxxxxxxxxxxxx',\n",
       " 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx',\n",
       " 'xxyyxx',\n",
       " 'xy',\n",
       " 'xyauyu',\n",
       " 'xyclo',\n",
       " 'xylitol',\n",
       " 'xylophone',\n",
       " 'xyugen',\n",
       " 'xyz',\n",
       " 'xzoanna',\n",
       " 'xào',\n",
       " 'xèo',\n",
       " 'ya',\n",
       " 'yaa',\n",
       " 'yaaa',\n",
       " 'yaaaa',\n",
       " 'yaaaaa',\n",
       " 'yaaaaaa',\n",
       " 'yaaaaaaaa',\n",
       " 'yaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaayyyyyyyyyyyyyy',\n",
       " 'yaaaaaaaaaaaassssss',\n",
       " 'yaaaaaaaaaahhhhhhhhhh',\n",
       " 'yaaaaaaaas',\n",
       " 'yaaaaaaaayyyyyy',\n",
       " 'yaaaaaassss',\n",
       " 'yaaaaaassssss',\n",
       " 'yaaaaaay',\n",
       " 'yaaaaas',\n",
       " 'yaaaaay',\n",
       " 'yaaaahhhhh',\n",
       " 'yaaaas',\n",
       " 'yaaaasss',\n",
       " 'yaaaassss',\n",
       " 'yaaaassssssssss',\n",
       " 'yaaaaummm',\n",
       " 'yaaaawn',\n",
       " 'yaaaay',\n",
       " 'yaaas',\n",
       " 'yaaassss',\n",
       " 'yaaay',\n",
       " 'yaah',\n",
       " 'yaan',\n",
       " 'yaang',\n",
       " 'yaanik',\n",
       " 'yaawwwkk',\n",
       " 'yaay',\n",
       " 'yaayy',\n",
       " 'yaayyyy',\n",
       " 'yabadabadoo',\n",
       " 'yabba',\n",
       " 'yabi',\n",
       " 'yabs',\n",
       " 'yabu',\n",
       " 'yac',\n",
       " 'yaca',\n",
       " 'yacajito',\n",
       " 'yacht',\n",
       " 'yachters',\n",
       " 'yachtsman',\n",
       " 'yachty',\n",
       " 'yack',\n",
       " 'yackin',\n",
       " 'yacky',\n",
       " 'yacu',\n",
       " 'yada',\n",
       " 'yadada',\n",
       " 'yadadah',\n",
       " 'yadayada',\n",
       " 'yadayadada',\n",
       " 'yadda',\n",
       " 'yadi',\n",
       " 'yadiyada',\n",
       " 'yaegaki',\n",
       " 'yaeger',\n",
       " 'yael',\n",
       " 'yaelle',\n",
       " 'yaf',\n",
       " 'yaffa',\n",
       " 'yager',\n",
       " 'yagerbomb',\n",
       " 'yagi',\n",
       " 'yagihashi',\n",
       " 'yago',\n",
       " 'yagura',\n",
       " 'yah',\n",
       " 'yahh',\n",
       " 'yahhhh',\n",
       " 'yahhhhh',\n",
       " 'yahhhhzzzz',\n",
       " 'yahitori',\n",
       " 'yahknit',\n",
       " 'yahng',\n",
       " 'yahoo',\n",
       " 'yahooligans',\n",
       " 'yahooo',\n",
       " 'yahoooo',\n",
       " 'yahrzeit',\n",
       " 'yahs',\n",
       " 'yahta',\n",
       " 'yahti',\n",
       " 'yahtzee',\n",
       " 'yahuh',\n",
       " 'yahy',\n",
       " 'yahya',\n",
       " 'yai',\n",
       " 'yaikitori',\n",
       " 'yailes',\n",
       " 'yaitori',\n",
       " 'yak',\n",
       " 'yakameia',\n",
       " 'yakamein',\n",
       " 'yakatori',\n",
       " 'yakauza',\n",
       " 'yake',\n",
       " 'yakee',\n",
       " 'yakers',\n",
       " 'yakety',\n",
       " 'yaki',\n",
       " 'yakii',\n",
       " 'yakima',\n",
       " 'yakimandu',\n",
       " 'yakimochi',\n",
       " 'yakin',\n",
       " 'yakinabe',\n",
       " 'yakinasu',\n",
       " 'yakiniku',\n",
       " 'yakinku',\n",
       " 'yakinuki',\n",
       " 'yakinuku',\n",
       " 'yakioni',\n",
       " 'yakionigiri',\n",
       " 'yakiori',\n",
       " 'yakiotri',\n",
       " 'yakiroti',\n",
       " 'yakisakana',\n",
       " 'yakisoba',\n",
       " 'yakitate',\n",
       " 'yakitoi',\n",
       " 'yakitori',\n",
       " 'yakitories',\n",
       " 'yakitoris',\n",
       " 'yakitoritotto',\n",
       " 'yakitoro',\n",
       " 'yakitorri',\n",
       " 'yakitory',\n",
       " 'yakittori',\n",
       " 'yakiudon',\n",
       " 'yakizakana',\n",
       " 'yakkk',\n",
       " 'yakko',\n",
       " 'yaknowaddamean',\n",
       " 'yaknowhati',\n",
       " 'yaknowwhadimean',\n",
       " 'yaknowwhutimsayin',\n",
       " 'yako',\n",
       " 'yakos',\n",
       " 'yakotori',\n",
       " 'yakov',\n",
       " 'yaktori',\n",
       " 'yaku',\n",
       " 'yakudori',\n",
       " 'yakult',\n",
       " 'yakuma',\n",
       " 'yakumi',\n",
       " 'yakuniku',\n",
       " 'yakut',\n",
       " 'yakuza',\n",
       " 'yakya',\n",
       " 'yakyudori',\n",
       " 'yal',\n",
       " 'yalanci',\n",
       " 'yalangi',\n",
       " 'yale',\n",
       " 'yaledailynews',\n",
       " 'yalelites',\n",
       " 'yalie',\n",
       " 'yalies',\n",
       " 'yall',\n",
       " 'yalla',\n",
       " 'yallies',\n",
       " 'yalls',\n",
       " 'yalo',\n",
       " 'yalp',\n",
       " 'yalper',\n",
       " 'yalumba',\n",
       " 'yam',\n",
       " 'yama',\n",
       " 'yamada',\n",
       " 'yamadaya',\n",
       " 'yamadora',\n",
       " 'yamagata',\n",
       " 'yamagobo',\n",
       " 'yamahai',\n",
       " 'yamahama',\n",
       " 'yamaimo',\n",
       " 'yamaka',\n",
       " 'yamakake',\n",
       " 'yamakakke',\n",
       " 'yamakul',\n",
       " 'yamamoto',\n",
       " 'yamamuto',\n",
       " 'yamao',\n",
       " 'yamasa',\n",
       " 'yamasaki',\n",
       " 'yamato',\n",
       " 'yamauchi',\n",
       " 'yamazak',\n",
       " 'yamazake',\n",
       " 'yamazaki',\n",
       " 'yami',\n",
       " 'yamiii',\n",
       " 'yamitsuki',\n",
       " 'yamitsuri',\n",
       " 'yammer',\n",
       " 'yammi',\n",
       " 'yammiiii',\n",
       " 'yammmi',\n",
       " 'yammmii',\n",
       " 'yammmiiii',\n",
       " 'yammmmiiii',\n",
       " 'yammmy',\n",
       " 'yammy',\n",
       " 'yamo',\n",
       " 'yamsayin',\n",
       " 'yan',\n",
       " 'yana',\n",
       " 'yandel',\n",
       " 'yandrel',\n",
       " 'yang',\n",
       " 'yangming',\n",
       " 'yangnyeom',\n",
       " 'yangon',\n",
       " 'yangrou',\n",
       " 'yangroupaomo',\n",
       " 'yangzhou',\n",
       " 'yangzi',\n",
       " 'yani',\n",
       " 'yanick',\n",
       " 'yank',\n",
       " 'yankee',\n",
       " 'yanker',\n",
       " 'yankin',\n",
       " 'yankkes',\n",
       " 'yankovic',\n",
       " 'yankspeak',\n",
       " 'yann',\n",
       " 'yanna',\n",
       " 'yanni',\n",
       " 'yannick',\n",
       " 'yannis',\n",
       " 'yanno',\n",
       " 'yannow',\n",
       " 'yano',\n",
       " 'yantacaw',\n",
       " 'yanticaw',\n",
       " 'yao',\n",
       " 'yaokui',\n",
       " 'yaooowwww',\n",
       " 'yaos',\n",
       " 'yaoung',\n",
       " 'yaoza',\n",
       " 'yap',\n",
       " 'yapp',\n",
       " 'yappers',\n",
       " 'yappin',\n",
       " 'yappy',\n",
       " 'yaprak',\n",
       " 'yaqk',\n",
       " 'yar',\n",
       " 'yard',\n",
       " 'yardbird',\n",
       " 'yardhouse',\n",
       " 'yardi',\n",
       " 'yardie',\n",
       " 'yardley',\n",
       " 'yardleyinn',\n",
       " 'yardly',\n",
       " 'yardsmobile',\n",
       " 'yardstick',\n",
       " 'yardville',\n",
       " 'yarg',\n",
       " 'yarghle',\n",
       " 'yarik',\n",
       " 'yarimar',\n",
       " 'yarissa',\n",
       " 'yariv',\n",
       " 'yarizza',\n",
       " 'yarmulke',\n",
       " 'yarn',\n",
       " 'yarng',\n",
       " 'yarnmade',\n",
       " 'yaron',\n",
       " 'yars',\n",
       " 'yartmr',\n",
       " 'yaryna',\n",
       " 'yas',\n",
       " 'yasada',\n",
       " 'yasai',\n",
       " 'yasaimori',\n",
       " 'yase',\n",
       " 'yaself',\n",
       " 'yaselves',\n",
       " 'yashuda',\n",
       " 'yasin',\n",
       " 'yasmin',\n",
       " 'yasmine',\n",
       " 'yasou',\n",
       " 'yass',\n",
       " 'yassa',\n",
       " 'yasser',\n",
       " 'yassir',\n",
       " 'yasss',\n",
       " 'yassss',\n",
       " 'yastremski',\n",
       " 'yasu',\n",
       " 'yasubee',\n",
       " 'yasuda',\n",
       " 'yasusa',\n",
       " 'yata',\n",
       " 'yatagan',\n",
       " 'yatagen',\n",
       " 'yatai',\n",
       " 'yatakilt',\n",
       " 'yater',\n",
       " 'yates',\n",
       " 'yateua',\n",
       " 'yatitori',\n",
       " 'yattai',\n",
       " 'yau',\n",
       " 'yauch',\n",
       " 'yaught',\n",
       " 'yautia',\n",
       " 'yaverm',\n",
       " 'yawaraka',\n",
       " 'yawarakani',\n",
       " 'yaweh',\n",
       " 'yawk',\n",
       " 'yawkah',\n",
       " 'yawkas',\n",
       " 'yawker',\n",
       " 'yawkers',\n",
       " 'yawn',\n",
       " 'yawner',\n",
       " 'yawny',\n",
       " 'yawp',\n",
       " 'yawwwn',\n",
       " 'yay',\n",
       " 'yaya',\n",
       " 'yayas',\n",
       " 'yayay',\n",
       " 'yayayay',\n",
       " 'yayayayayay',\n",
       " 'yaye',\n",
       " 'yayla',\n",
       " 'yayo',\n",
       " 'yayoi',\n",
       " 'yayuh',\n",
       " 'yayy',\n",
       " 'yayyayayayayayayayaay',\n",
       " 'yayyayyayayyayayyayyayyay',\n",
       " 'yayyy',\n",
       " 'yayyyy',\n",
       " 'yayyyyy',\n",
       " 'yayyyyyy',\n",
       " 'yayyyyyyy',\n",
       " 'yaz',\n",
       " 'yazici',\n",
       " 'yazmin',\n",
       " 'yazu',\n",
       " 'yb',\n",
       " 'ybf',\n",
       " 'ybn',\n",
       " 'ybnyc',\n",
       " 'ybor',\n",
       " 'yc',\n",
       " 'ycdf',\n",
       " 'yclvb',\n",
       " 'yco',\n",
       " 'yd',\n",
       " 'yday',\n",
       " 'ydrp',\n",
       " 'yds',\n",
       " 'ydtr',\n",
       " 'ye',\n",
       " 'yea',\n",
       " ...]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(review4.lemma)\n",
    "sorted(vectorizer.get_feature_names_out()[-3500:-2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some strange words written by reviewers are still remained: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aaa',\n",
       " 'aaaa',\n",
       " 'aaaaa',\n",
       " 'aaaaaa',\n",
       " 'aaaaaaaaaaaa',\n",
       " 'aaaaaaaaaaaaaa',\n",
       " 'aaaaaaaaaaaaaaaaaa',\n",
       " 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'aaaaaaaaaaaaaaaasaaaaaaaaaammmmmmmmmmmmmaaaaaaaaaa',\n",
       " 'aaaaaaaaaaaaaand',\n",
       " 'aaaaaaaaaaaaah',\n",
       " 'aaaaaaaaaaaallll',\n",
       " 'aaaaaaaaaaaamazing',\n",
       " 'aaaaaaaaaaamazing',\n",
       " 'aaaaaaaaaarrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr',\n",
       " 'aaaaaaaaamazing',\n",
       " 'aaaaaaaaand',\n",
       " 'aaaaaaaahh']"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vectorizer.get_feature_names_out()[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention: some of the reviews are written in other languages, such as French, Chinese, Japanese..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['腌笃鲜',\n",
       " '臊子面不到刀',\n",
       " '菜品也好',\n",
       " '葱油饼',\n",
       " '薄い',\n",
       " '虽然他们比较忙',\n",
       " '蝦米腸粉',\n",
       " '蝦餃',\n",
       " '蟹粉小笼包',\n",
       " '装修也好',\n",
       " '西安美食还有成都美食',\n",
       " '要是老板可以改名字就更好了',\n",
       " '许多清淡的菜味道一样的棒',\n",
       " '豆瓣魚',\n",
       " '超正',\n",
       " '超軽量といった魅了する',\n",
       " '软骨',\n",
       " '辛いので注意',\n",
       " '还好电话没变',\n",
       " '还问我们要不要水什么的',\n",
       " '这个吃货来讲根本是无法抵抗呀',\n",
       " '这么俗的店名差点就让我错过了这里的美食',\n",
       " '这些菜还没自己在家里做的好吃也是醉了',\n",
       " '这家餐厅就像一碗甜美的汤',\n",
       " '这家餐馆相当的棒',\n",
       " '进去可以直接说普通话了',\n",
       " '這家餐廳非常好',\n",
       " '都很好',\n",
       " '酸的很奇怪',\n",
       " '酸菜',\n",
       " '酸辣鸡杂',\n",
       " '醋溜土豆丝用白醋',\n",
       " '里面是芝士加牛肉的也特别好吃',\n",
       " '量は特盛クラス',\n",
       " '銀針粉',\n",
       " '门口还有好几个排队',\n",
       " '雲丹の塩昆布焼き',\n",
       " '非常的脆',\n",
       " '非常糟糕的服务',\n",
       " '面包硬的挑战你的口腔',\n",
       " '順番がきたらチキンオーバーライスと告げ',\n",
       " '顺带一提餐厅的装修我很喜欢',\n",
       " '食べ物はとても良いです',\n",
       " '食物多为冷藏加热',\n",
       " '首先菜不新鲜',\n",
       " '魚香茄子雞',\n",
       " '鱼片和手指头差不多厚',\n",
       " '麻婆豆腐也不錯',\n",
       " '黑木耳',\n",
       " '黒糖梅酒']"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vectorizer.get_feature_names_out()[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  prod_id  rating  label  \\\n",
      "0     5044        0     1.0     -1   \n",
      "1     5045        0     1.0     -1   \n",
      "2     5046        0     3.0     -1   \n",
      "3     5047        0     5.0     -1   \n",
      "4     5048        0     5.0     -1   \n",
      "\n",
      "                                               lemma  \n",
      "0  ['drink', 'bad', 'hot', 'chocolate', 'water', ...  \n",
      "1  ['bad', 'experience', 'ever', 'casual', 'coffe...  \n",
      "2  ['locate', 'site', 'old', 'spruce', 'street', ...  \n",
      "3  ['enjoy', 'coffee', 'breakfast', 'twice', 'toa...  \n",
      "4  ['love', 'toast', 'food', 'choice', 'fantastic...  \n"
     ]
    }
   ],
   "source": [
    "review4=review4[['user_id', 'prod_id', 'rating', 'label','lemma']]\n",
    "print(review4.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) create number of words, number of verbs, average word length, and emotiveness ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 52min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "length=len(review4)\n",
    "empty_col=np.empty([length,4])\n",
    "review4=np.append(review4,empty_col,1)\n",
    "for i in range(length):\n",
    "    text = review4[i, 4]\n",
    "    text=text[1:-1]\n",
    "    text=text.replace(\"'\",\"\")\n",
    "    text=text.split(\",\")\n",
    "    text=[w.strip() for w in text]\n",
    "\n",
    "    num_of_words=len(text)\n",
    "    word_tag = pos_tag(text)\n",
    "    num_of_verb=0\n",
    "    num_of_adj=0\n",
    "    num_of_adv=0\n",
    "    num_of_noun=0\n",
    "    sum_len_word=0\n",
    "    for word, tag in word_tag:\n",
    "        len_word=len(word)\n",
    "        sum_len_word += len_word\n",
    "\n",
    "        tag = tag[0:2].lower()\n",
    "        if tag==\"vb\":\n",
    "            num_of_verb+=1\n",
    "        elif tag==\"jj\":\n",
    "            num_of_adj+=1\n",
    "        elif tag==\"rb\":\n",
    "            num_of_adv+=1\n",
    "        elif tag==\"nn\":\n",
    "            num_of_noun+=1\n",
    "\n",
    "    avg_word_len=sum_len_word/len(text)\n",
    "    review4[i,5]=num_of_words\n",
    "    review4[i,6]=num_of_verb\n",
    "    review4[i,7]=avg_word_len\n",
    "\n",
    "    if num_of_noun+num_of_verb!=0:\n",
    "        emotiveness_ratio=(num_of_adj+num_of_adv)/(num_of_noun+num_of_verb)\n",
    "    else:\n",
    "        emotiveness_ratio=0\n",
    "    review4[i,8]=emotiveness_ratio\n",
    "    #print(num_of_words,num_of_verb,avg_word_len,emotiveness_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "savetxt(\"review_features_01.txt\",review4,fmt=\"%s\",delimiter=\"\\t\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) create number of positive words, number of negative words and sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  prod_id  rating  label  \\\n",
      "0     5044        0     1.0     -1   \n",
      "1     5045        0     1.0     -1   \n",
      "2     5046        0     3.0     -1   \n",
      "3     5047        0     5.0     -1   \n",
      "4     5048        0     5.0     -1   \n",
      "\n",
      "                                          list_words  num_of_words  \\\n",
      "0  ['drink', 'bad', 'hot', 'chocolate', 'water', ...            17   \n",
      "1  ['bad', 'experience', 'ever', 'casual', 'coffe...           115   \n",
      "2  ['locate', 'site', 'old', 'spruce', 'street', ...            24   \n",
      "3  ['enjoy', 'coffee', 'breakfast', 'twice', 'toa...           128   \n",
      "4  ['love', 'toast', 'food', 'choice', 'fantastic...            77   \n",
      "\n",
      "   num_of_verbs  avg_word_len  emotiveness  \n",
      "0             2      4.882353     0.416667  \n",
      "1            17      5.452174     0.379747  \n",
      "2             3      5.291667     0.600000  \n",
      "3            15      5.664062     0.465116  \n",
      "4            11      5.337662     0.520000  \n",
      "length of data: 608598\n",
      "check the null data: False\n"
     ]
    }
   ],
   "source": [
    "file_review5=\"D:/Lu Yu/review_features_01.txt\"\n",
    "review5=pd.read_csv(file_review5,sep=\"\\t\",header=None)\n",
    "review5.columns=['user_id', 'prod_id', 'rating', 'label','list_words','num_of_words',\n",
    "                 'num_of_verbs','avg_word_len','emotiveness']\n",
    "print(review5.head())\n",
    "print(\"length of data:\",len(review5))\n",
    "print(\"check the null data:\",review5.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_positive=\"C:/Users/Lu/Desktop/Opinion Lexicon/positive-words.txt\"\n",
    "file_negative=\"C:/Users/Lu/Desktop/Opinion Lexicon/negative-words.txt\"\n",
    "positive_list=open(file_positive,\"r\").read().split()\n",
    "negative_list=open(file_negative,\"r\").read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_col2=np.empty([length,3])\n",
    "review5=np.append(review5,empty_col2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4h 35min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(length):\n",
    "    num_positive=0\n",
    "    num_negative=0\n",
    "    text = review5[i, 4]\n",
    "    text=text[1:-1]\n",
    "    text=text.replace(\"'\",\"\")\n",
    "    text=text.split(\",\")\n",
    "    text=[w.strip() for w in text]\n",
    "    len_text=len(text)\n",
    "    for m in text:\n",
    "        for n in positive_list:\n",
    "            if m==n:\n",
    "                num_positive+=1\n",
    "        for k in negative_list:\n",
    "            if m==k:\n",
    "                num_negative+=1\n",
    "    sentiment=(num_positive-num_negative)/len_text\n",
    "    review5[i,9]=num_positive\n",
    "    review5[i,10]=num_negative\n",
    "    review5[i,11]=sentiment\n",
    "# print(review5[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5044 0 1.0 -1\n",
      "  \"['drink', 'bad', 'hot', 'chocolate', 'water', 'latte', 'burn', 'taste', 'food', 'also', 'poor', 'quality', 'service', 'bad', 'part', 'cashier', 'rude']\"\n",
      "  17 2 4.882352941176471 0.4166666666666667 1 5 -0.23529411764705882]\n",
      " [5045 0 1.0 -1\n",
      "  \"['bad', 'experience', 'ever', 'casual', 'coffee', 'light', 'fare', 'place', 'server', 'disappear', 'minute', 'talk', 'friend', 'window', 'girlfriend', 'sit', 'dumbfound', 'dude', 'nerve', 'job', 'try', 'make', 'eye', 'contact', 'clearly', 'get', 'pay', 'talk', 'bud', 'important', 'girlfriend', 'go', 'counter', 'server', 'disappear', 'back', 'another', 'minute', 'guy', 'ask', 'order', 'food', 'something', 'girl', 'counter', 'give', 'weird', 'look', 'say', 'get', 'server', 'arrive', 'back', 'look', 'table', 'laugh', 'yeah', 'leave', 'u', 'hang', 'half', 'goddamn', 'hour', 'place', 'two', 'customer', 'funny', 'retrospect', 'collective', 'incompetence', 'false', 'sense', 'entitlement', 'certainly', 'food', 'okay', 'place', 'call', 'toast', 'figure', 'bread', 'would', 'good', 'cold', 'le', 'bus', 'additionally', 'sure', 'andouille', 'special', 'link', 'pre', 'package', 'offer', 'trader', 'joe', 'cut', 'four', 'piece', 'unapologetic', 'mediocrity', 'happen', 'avoid', 'place', 'like', 'plague', 'almost', 'leave', 'tip', 'honestly', 'felt', 'buyer', 'remorse', 'day', 'disgrace']\"\n",
      "  115 17 5.452173913043478 0.379746835443038 4 12 -0.06956521739130435]]\n"
     ]
    }
   ],
   "source": [
    "print(review5[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "savetxt(\"review_features_02.txt\",review5,fmt=\"%s\",delimiter=\"\\t\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) create lexical diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id                                              lemma\n",
      "0     5044  ['drink', 'bad', 'hot', 'chocolate', 'water', ...\n",
      "1     5045  ['bad', 'experience', 'ever', 'casual', 'coffe...\n",
      "2     5046  ['locate', 'site', 'old', 'spruce', 'street', ...\n",
      "3     5047  ['enjoy', 'coffee', 'breakfast', 'twice', 'toa...\n",
      "4     5048  ['love', 'toast', 'food', 'choice', 'fantastic...\n"
     ]
    }
   ],
   "source": [
    "file_review6=\"D:/Lu Yu/orig_review_with_labeling_608598rows_af_lemma.txt\"\n",
    "review6=pd.read_csv(file_review6,sep=\"\\t\",header=None)\n",
    "review6.columns=['user_id', 'prod_id', 'date', 'review', 'rating', 'label','lemma']\n",
    "review6=review6[['user_id','lemma']]\n",
    "print(review6.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(608598, 3)\n"
     ]
    }
   ],
   "source": [
    "import lexical_diversity\n",
    "from lexical_diversity import lex_div as ld\n",
    "length=len(review6)\n",
    "df=review6.copy()\n",
    "empty_col=np.empty([length,1])\n",
    "df=np.append(df,empty_col,1)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(length):\n",
    "    text = df[i, 1]\n",
    "    text=text[1:-1]\n",
    "    text=text.replace(\"'\",\"\")\n",
    "    text=text.split(\",\")\n",
    "    text=[w.strip() for w in text]\n",
    "    #TTR: the ratio of the total number of different words in a language sample to the total number of words in the sample\n",
    "    TTR=ld.ttr(text)\n",
    "    df[i,2]=TTR\n",
    "df=np.delete(df,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5044 0.9411764705882353]\n",
      " [5045 0.8695652173913043]]\n"
     ]
    }
   ],
   "source": [
    "print(df[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "savetxt(\"feature_lexical_diversity.txt\",df,fmt=\"%s\",delimiter=\"\\t\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  lexical_diversity\n",
      "0     5044           0.941176\n",
      "1     5045           0.869565\n",
      "2     5046           0.916667\n",
      "3     5047           0.750000\n",
      "4     5048           0.831169\n"
     ]
    }
   ],
   "source": [
    "file_lexical=\"C:/Users/Lu/feature_lexical_diversity.txt\"\n",
    "lex_diversity=pd.read_csv(file_lexical,sep=\"\\t\",header=None)\n",
    "lex_diversity.columns=['user_id', 'lexical_diversity']\n",
    "print(lex_diversity.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) create typo ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id                                             review\n",
      "0     5044  Drinks were bad, the hot chocolate was watered...\n",
      "1     5045  This was the worst experience I've ever had a ...\n",
      "2     5046  This is located on the site of the old Spruce ...\n",
      "3     5047  I enjoyed coffee and breakfast twice at Toast ...\n",
      "4     5048  I love Toast! The food choices are fantastic -...\n"
     ]
    }
   ],
   "source": [
    "file_review7=\"D:/Lu Yu/orig_review_with_labeling_608598rows_af_lemma.txt\"\n",
    "review7=pd.read_csv(file_review6,sep=\"\\t\",header=None)\n",
    "review7.columns=['user_id', 'prod_id', 'date', 'review', 'rating', 'label','lemma']\n",
    "df_typo=review7.copy()[['user_id','review']]\n",
    "print(df_typo.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from spellchecker import SpellChecker\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "length=len(df_typo)\n",
    "empty_col=np.empty([length,1])\n",
    "df_typo=np.append(df_typo,empty_col,1)\n",
    "spell = SpellChecker()\n",
    "\n",
    "for i in range(len(df_typo)):\n",
    "    # a.lower the text\n",
    "    text=df_typo[i,1].lower()\n",
    "\n",
    "    # b.contraction words\n",
    "    text = text.replace(\"can't\", \"can not\")\n",
    "    text = text.replace(\"won't\", \"will not\")\n",
    "    text = text.replace(\"'ve\",\" have\")\n",
    "    text = text.replace(\"'d\",\" had\")\n",
    "    text = text.replace(\"'m\", \" am\")\n",
    "    text = text.replace(\"'ll\", \" will\")\n",
    "    text = text.replace(\"'s\", \" is\")\n",
    "    text = text.replace(\"n't\", \" not\")\n",
    "    text = text.replace(\"'re\", \" are\")\n",
    "    text = text.replace(\"st.\", \"street\")\n",
    "    text = text.replace(\"bldg.\", \"building\") \n",
    "    \n",
    "    # c.deal with punctuation such as ‘!”#$%&'()*+,-./:;?@[\\]^_`{|}~’, and including \"...\",\"???\"...\n",
    "    text=re.sub(r\"[^\\w\\s]\", \" \", text) \n",
    "    \n",
    "    # d.remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    \n",
    "    text = text.split()\n",
    "\n",
    "    len_word=len(text)\n",
    "    if len_word==0:\n",
    "        typo_ratio=0\n",
    "    else:\n",
    "        misspelled = list(spell.unknown(text))\n",
    "        typo_ratio=len(misspelled)/len_word\n",
    "    df_typo[i,2]=typo_ratio\n",
    "\n",
    "df_typo=np.delete(df_typo,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5044 0.0]\n",
      " [5045 0.0]\n",
      " [5046 0.04]\n",
      " [5047 0.008547008547008548]\n",
      " [5048 0.0]]\n"
     ]
    }
   ],
   "source": [
    "print(df_typo[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "savetxt(\"feature_typo_ratio.txt\",df_typo,fmt=\"%s\",delimiter=\"\\t\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  typo_ratio\n",
      "0     5044    0.000000\n",
      "1     5045    0.000000\n",
      "2     5046    0.040000\n",
      "3     5047    0.008547\n",
      "4     5048    0.000000\n"
     ]
    }
   ],
   "source": [
    "file_typo=\"D:/Lu Yu/feature_typo_ratio.txt\"\n",
    "typo=pd.read_csv(file_typo,sep=\"\\t\",header=None)\n",
    "typo.columns=['user_id', 'typo_ratio']\n",
    "print(typo.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) baseline svm model without TF-IGF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  prod_id  rating  label  \\\n",
      "0     5044        0     1.0     -1   \n",
      "1     5045        0     1.0     -1   \n",
      "2     5046        0     3.0     -1   \n",
      "3     5047        0     5.0     -1   \n",
      "4     5048        0     5.0     -1   \n",
      "\n",
      "                                          list_words  num_of_words  \\\n",
      "0  ['drink', 'bad', 'hot', 'chocolate', 'water', ...            17   \n",
      "1  ['bad', 'experience', 'ever', 'casual', 'coffe...           115   \n",
      "2  ['locate', 'site', 'old', 'spruce', 'street', ...            24   \n",
      "3  ['enjoy', 'coffee', 'breakfast', 'twice', 'toa...           128   \n",
      "4  ['love', 'toast', 'food', 'choice', 'fantastic...            77   \n",
      "\n",
      "   num_of_verbs  avg_word_len  emotiveness  num_positive  num_negative  \\\n",
      "0             2      4.882353     0.416667             1             5   \n",
      "1            17      5.452174     0.379747             4            12   \n",
      "2             3      5.291667     0.600000             4             1   \n",
      "3            15      5.664062     0.465116            20             4   \n",
      "4            11      5.337662     0.520000            12             1   \n",
      "\n",
      "   sentiment  \n",
      "0  -0.235294  \n",
      "1  -0.069565  \n",
      "2   0.125000  \n",
      "3   0.125000  \n",
      "4   0.142857  \n"
     ]
    }
   ],
   "source": [
    "file_review8=\"D:/Lu Yu/review_features_02.txt\"\n",
    "review8=pd.read_csv(file_review8,sep=\"\\t\",header=None)\n",
    "review8.columns=['user_id', 'prod_id', 'rating', 'label','list_words','num_of_words',\n",
    "                 'num_of_verbs','avg_word_len','emotiveness','num_positive','num_negative','sentiment']\n",
    "print(review8.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  prod_id  rating  label  \\\n",
      "0     5044        0     1.0     -1   \n",
      "1     5045        0     1.0     -1   \n",
      "2     5046        0     3.0     -1   \n",
      "3     5047        0     5.0     -1   \n",
      "4     5048        0     5.0     -1   \n",
      "\n",
      "                                          list_words  num_of_words  \\\n",
      "0  ['drink', 'bad', 'hot', 'chocolate', 'water', ...            17   \n",
      "1  ['bad', 'experience', 'ever', 'casual', 'coffe...           115   \n",
      "2  ['locate', 'site', 'old', 'spruce', 'street', ...            24   \n",
      "3  ['enjoy', 'coffee', 'breakfast', 'twice', 'toa...           128   \n",
      "4  ['love', 'toast', 'food', 'choice', 'fantastic...            77   \n",
      "\n",
      "   num_of_verbs  avg_word_len  emotiveness  num_positive  num_negative  \\\n",
      "0             2      4.882353     0.416667             1             5   \n",
      "1            17      5.452174     0.379747             4            12   \n",
      "2             3      5.291667     0.600000             4             1   \n",
      "3            15      5.664062     0.465116            20             4   \n",
      "4            11      5.337662     0.520000            12             1   \n",
      "\n",
      "   sentiment  lexical_diversity  typo_ratio  \n",
      "0  -0.235294           0.941176    0.000000  \n",
      "1  -0.069565           0.869565    0.000000  \n",
      "2   0.125000           0.916667    0.040000  \n",
      "3   0.125000           0.750000    0.008547  \n",
      "4   0.142857           0.831169    0.000000  \n"
     ]
    }
   ],
   "source": [
    "preprocessed_data=review8.copy()\n",
    "preprocessed_data['lexical_diversity']=lex_diversity['lexical_diversity']\n",
    "preprocessed_data['typo_ratio']=typo['typo_ratio']\n",
    "print(preprocessed_data.head())\n",
    "filtered_features=['rating','num_of_words', 'num_of_verbs','avg_word_len',\n",
    "                   'emotiveness','num_positive','num_negative','sentiment',\n",
    "                   'lexical_diversity','typo_ratio']\n",
    "\n",
    "label_col='label'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426018\n",
      "426018\n"
     ]
    }
   ],
   "source": [
    "baseline_svm = LinearSVC()\n",
    "scaler = StandardScaler()\n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_data[filtered_features],\n",
    "                                                    preprocessed_data[label_col].values,\n",
    "                                                    test_size=.3,\n",
    "                                                    random_state=1)\n",
    "resampler = SMOTE(random_state=24, k_neighbors=3)\n",
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\envs\\Python37\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('upsampler', SMOTE(k_neighbors=3, random_state=24)),\n",
       "                ('svc', LinearSVC())])"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "svm_pipe = Pipeline([('scaler', scaler),\n",
    "                     ('upsampler', resampler),\n",
    "                     ('svc', baseline_svm)])\n",
    "svm_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = svm_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07793757,  0.29522536, -0.14246885, -0.01534178,  0.00939956,\n",
       "         0.09379281, -0.08311056, -0.10143029, -0.00699425, -0.01839403]])"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_pipe[2].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.18      0.66      0.28     24078\n",
      "           1       0.91      0.53      0.67    158502\n",
      "\n",
      "    accuracy                           0.55    182580\n",
      "   macro avg       0.54      0.60      0.47    182580\n",
      "weighted avg       0.81      0.55      0.62    182580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15916  8162]\n",
      " [74439 84063]]\n"
     ]
    }
   ],
   "source": [
    "CM=confusion_matrix(y_test,preds)\n",
    "print(CM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
