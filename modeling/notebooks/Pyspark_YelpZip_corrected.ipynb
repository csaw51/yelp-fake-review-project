{"cells":[{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.functions import udf, col, lower, regexp_replace\n\nfile_review=\"/FileStore/tables/selected_features_for_pyspark.txt\"\nsc = SparkContext.getOrCreate()\nspark = SQLContext(sc)\n\ndf=spark.read.option(\"sep\", \"\\t\").csv(file_review)\n#df.show(5)\n\ndf=df.select(col(\"_c0\").cast(\"int\").alias(\"user_id\"),col(\"_c1\").cast(\"int\").alias(\"prod_id\"),col(\"_c2\").cast(\"string\").alias(\"date\"),\n                 col(\"_c3\").cast(\"float\").alias(\"rating\"),col(\"_c4\").cast(\"int\").alias(\"num_of_words\"),col(\"_c5\").cast(\"int\").alias(\"num_of_verbs\"),\n                 col(\"_c6\").cast(\"float\").alias(\"avg_word_length\"),col(\"_c7\").cast(\"float\").alias(\"emotiveness_ratio\"),col(\"_c8\").cast(\"int\").alias(\"num_of_positive\")\n                     ,col(\"_c9\").cast(\"int\").alias(\"num_of_negative\"), col(\"_c10\").cast(\"float\").alias(\"sentiment\"),col(\"_c11\").cast(\"int\").alias(\"label\"))\ndf=df.select(col(\"rating\"),col(\"num_of_words\"),col(\"num_of_verbs\"),col(\"avg_word_length\"),col(\"emotiveness_ratio\"),col(\"num_of_positive\"),\n                   col(\"num_of_negative\"),col(\"sentiment\"),col(\"label\"))\n\n# df=df.select(col(\"_c0\").cast(\"int\").alias(\"num_of_words\"),col(\"_c1\").cast(\"int\").alias(\"num_of_verbs\"),col(\"_c2\").cast(\"float\").alias(\"avg_word_length\"),\n#             col(\"_c3\").cast(\"float\").alias(\"emotiveness_ratio\"),col(\"_c4\").cast(\"int\").alias(\"num_of_posi\"),col(\"_c5\").cast(\"int\").alias(\"num_of_nega\"),\n#             col(\"_c6\").cast(\"float\").alias(\"sentiment\"),col(\"_c7\").cast(\"int\").alias(\"label\"))\n#data.show(5)\n\nfrom pyspark.sql.functions import when\ndf=df.withColumn(\"label\",when(df.label==-1,0).otherwise(1))\ndf=df.toPandas()\ndf=df.iloc[0:50000,:]\nprint(len(df))\n#print(df.head())\n\ndf = spark.createDataFrame(df)\nprint(\"total real review numbers:\",df[df.label==1].count())\nprint(\"total fake review numbers:\",df[df.label==0].count())\nprint(\"total fake review percentage:\",df[df.label==0].count()/df[df.label==1].count())\n#df.show(20)\n#df=df[0:10000]\nfrom pyspark.ml.feature import MinMaxScaler\nfrom pyspark.ml.classification import LinearSVC\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.feature import VectorAssembler\nfrom sklearn.metrics import confusion_matrix\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import DoubleType\nimport numpy as np\nimport pandas as pd\n#------------------------------------------scaling start--------------------------------------------------------------------------------------\nunlist = udf(lambda x: round(float(list(x)[0]),3), DoubleType())\nfor i in [\"rating\",\"num_of_words\",\"num_of_verbs\",\"avg_word_length\",\"emotiveness_ratio\",\"num_of_positive\",\"num_of_negative\",\"sentiment\"]:\n    # VectorAssembler Transformation - Converting column to vector type\n    assembler = VectorAssembler(inputCols=[i],outputCol=i+\"_Vect\")\n\n    # MinMaxScaler Transformation\n    scaler = MinMaxScaler(inputCol=i+\"_Vect\", outputCol=i+\"_Scaled\")\n\n    # Pipeline of VectorAssembler and MinMaxScaler\n    pipeline = Pipeline(stages=[assembler, scaler])\n\n    # Fitting pipeline on dataframe\n    df = pipeline.fit(df).transform(df).withColumn(i+\"_Scaled\", unlist(i+\"_Scaled\")).drop(i+\"_Vect\")\ndf=df.select([\"rating_Scaled\",\"num_of_words_Scaled\",\"num_of_verbs_Scaled\",\"avg_word_length_Scaled\",\"emotiveness_ratio_Scaled\",\"num_of_positive_Scaled\",\"num_of_negative_Scaled\",\"sentiment_Scaled\",\"label\"])\n#df.show(10)\n#------------------------------------------scaling end----------------------------------------------------------------------------------------\nfeatures=np.array([\"rating_Scaled\",\"num_of_words_Scaled\",\"num_of_verbs_Scaled\",\"avg_word_length_Scaled\",\"emotiveness_ratio_Scaled\",\"num_of_positive_Scaled\",\"num_of_negative_Scaled\",\"sentiment_Scaled\"])\n\n#print(type(features))\nva=VectorAssembler(inputCols = features, outputCol='features')\nva_df = va.transform(df)\nva_df = va_df.select([\"features\",\"label\"])\n#va_df.show(20)\n#train=va_df.filter(va_df.rand<0.7)\n#test=df.filter(va_df.rand>=0.7)\n(train, test) = va_df.randomSplit([0.7, 0.3],seed=2)\n#train.show(100)\nprint(\"total review numbers in training:\", train.count())\nprint(\"total review numbers in testing\", test.count())\nprint(\"real review numbers in training:\",train[train.label==1].count())\nprint(\"real review numbers in testing:\",test[test.label==1].count())\n# train.show()\n# test.show()\n# test_to_pd=test.toPandas()\n# print(test_to_pd.iloc[0:100,:])\nlsvc = LinearSVC(labelCol=\"label\", maxIter=50)\nlsvc = lsvc.fit(train)\n\npred = lsvc.transform(test)\npred.show(5)\nevaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\")\naccuracy = evaluator.evaluate(pred)\n \nprint(\"accuracy score: \", accuracy)\n\ny_pred=pred.select(\"prediction\").collect()\ny_orig=pred.select(\"label\").collect()\n\ntp = pred[(pred.label == 1) & (pred.prediction == 1)].count()\ntn = pred[(pred.label == 0) & (pred.prediction == 0)].count()\nfp = pred[(pred.label == 0) & (pred.prediction == 1)].count()\nfn = pred[(pred.label == 1) & (pred.prediction == 0)].count()\nfake_review_count=pred[pred.label==0].count()\nfake_review_02=test[test.label==0].count()\nprint(fake_review_count)\nprint(fake_review_02)\nprint(\"tp:\",tp)\nprint(\"tn:\",tn)\nprint(\"fp:\",fp)\nprint(\"fn:\",fn)\n\ncm = confusion_matrix(y_orig, y_pred)\nprint(\"Confusion Matrix:\")\nprint(cm) "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f2afeb51-b06d-495e-a7e8-ac3559619b70","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["/databricks/spark/python/pyspark/sql/context.py:82: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n50000\ntotal real review numbers: 43063\ntotal fake review numbers: 6937\ntotal fake review percentage: 0.1610895664491559\n35100\n14900\nreal review numbers in training: 30229\nreal review numbers in testing: 12834\n+--------------------+-----+--------------------+----------+\n|            features|label|       rawPrediction|prediction|\n+--------------------+-----+--------------------+----------+\n|(8,[0,1,3,7],[0.2...|    1|[-1.0000956356031...|       1.0|\n|(8,[0,1,3,7],[0.5...|    1|[-1.0000940943682...|       1.0|\n|(8,[0,1,3,7],[1.0...|    0|[-1.0000690306545...|       1.0|\n|(8,[0,1,3,7],[1.0...|    0|[-1.0000269087807...|       1.0|\n|(8,[0,3,5,7],[0.7...|    0|[-0.9998020314576...|       1.0|\n+--------------------+-----+--------------------+----------+\nonly showing top 5 rows\n\naccuracy score:  0.8613422818791946\n2066\n2066\ntp: 12834\ntn: 0\nfp: 2066\nfn: 0\nConfusion Matrix:\n[[    0  2066]\n [    0 12834]]\n"]}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Pyspark_YelpZip","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3801813559618754}},"nbformat":4,"nbformat_minor":0}
