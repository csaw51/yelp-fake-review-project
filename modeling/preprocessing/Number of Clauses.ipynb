{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "648120aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59731d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  prod_id        date  \\\n",
      "0     5044        0  2014-11-16   \n",
      "1     5045        0  2014-09-08   \n",
      "2     5046        0  2013-10-06   \n",
      "3     5047        0  2014-11-30   \n",
      "4     5048        0  2014-08-28   \n",
      "\n",
      "                                              review  rating  label  \n",
      "0  Drinks were bad, the hot chocolate was watered...     1.0     -1  \n",
      "1  This was the worst experience I've ever had a ...     1.0     -1  \n",
      "2  This is located on the site of the old Spruce ...     3.0     -1  \n",
      "3  I enjoyed coffee and breakfast twice at Toast ...     5.0     -1  \n",
      "4  I love Toast! The food choices are fantastic -...     5.0     -1  \n",
      "length of data: 608598\n",
      "check the null data: False\n"
     ]
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d05a45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review\n",
      "0  Drinks were bad, the hot chocolate was watered...\n",
      "1  This was the worst experience I've ever had a ...\n",
      "2  This is located on the site of the old Spruce ...\n",
      "3  I enjoyed coffee and breakfast twice at Toast ...\n",
      "4  I love Toast! The food choices are fantastic -...\n"
     ]
    }
   ],
   "source": [
    "review=review3[['review']]\n",
    "print(review.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c6946c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.5.1-cp39-cp39-win_amd64.whl (12.2 MB)\n",
      "Requirement already satisfied: jinja2 in f:\\anaconda\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp39-cp39-win_amd64.whl (18 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in f:\\anaconda\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in f:\\anaconda\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in f:\\anaconda\\lib\\site-packages (from spacy) (2.27.1)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.6-cp39-cp39-win_amd64.whl (482 kB)\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Using cached typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in f:\\anaconda\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: setuptools in f:\\anaconda\\lib\\site-packages (from spacy) (61.2.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.7-cp39-cp39-win_amd64.whl (2.2 MB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.9-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp39-cp39-win_amd64.whl (96 kB)\n",
      "Collecting pathy>=0.10.0\n",
      "  Using cached pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Using cached wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in f:\\anaconda\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Collecting typing-extensions>=4.2.0\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in f:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in f:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Using cached confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp39-cp39-win_amd64.whl (7.0 MB)\n",
      "Requirement already satisfied: colorama in f:\\anaconda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in f:\\anaconda\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Collecting colorama\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in f:\\anaconda\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: typing-extensions, colorama, catalogue, srsly, pydantic, murmurhash, cymem, wasabi, typer, smart-open, preshed, confection, blis, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.1.1\n",
      "    Uninstalling typing-extensions-4.1.1:\n",
      "      Successfully uninstalled typing-extensions-4.1.1\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.4\n",
      "    Uninstalling colorama-0.4.4:\n",
      "      Successfully uninstalled colorama-0.4.4\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 5.1.0\n",
      "    Uninstalling smart-open-5.1.0:\n",
      "      Successfully uninstalled smart-open-5.1.0\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 colorama-0.4.6 confection-0.0.4 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.7 smart-open-6.3.0 spacy-3.5.1 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.9 typer-0.7.0 typing-extensions-4.5.0 wasabi-1.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c58379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f576092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in f:\\anaconda\\lib\\site-packages (from en-core-web-sm==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in f:\\anaconda\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in f:\\anaconda\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.21.5)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in f:\\anaconda\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in f:\\anaconda\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in f:\\anaconda\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in f:\\anaconda\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in f:\\anaconda\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in f:\\anaconda\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in f:\\anaconda\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in f:\\anaconda\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in f:\\anaconda\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
      "Requirement already satisfied: jinja2 in f:\\anaconda\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.11.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in f:\\anaconda\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: setuptools in f:\\anaconda\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (61.2.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in f:\\anaconda\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in f:\\anaconda\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in f:\\anaconda\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in f:\\anaconda\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in f:\\anaconda\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in f:\\anaconda\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in f:\\anaconda\\lib\\site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in f:\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in f:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in f:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2021.10.8)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in f:\\anaconda\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in f:\\anaconda\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: colorama in f:\\anaconda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in f:\\anaconda\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in f:\\anaconda\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.5.0\n",
      "\u001b[38;5;3m[!] As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use\n",
      "the full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6040177",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5951c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"very rude\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b93fe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "4bc51fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "length=len(review)\n",
    "df=review.copy()\n",
    "df['num_clauses']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a99e05c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_clauses(doc):\n",
    "    print(doc)\n",
    "    if len(doc)!=0 and doc!=None:\n",
    "        def find_root_of_sentence(doc):\n",
    "            root_token = None\n",
    "            for token in doc:\n",
    "                if (token.dep_ == \"ROOT\"):\n",
    "                    root_token = token\n",
    "            return root_token\n",
    "        root_token = find_root_of_sentence(doc)\n",
    "        print(root_token)\n",
    "        def find_other_verbs(doc, root_token):\n",
    "            other_verbs = []\n",
    "            for token in doc:\n",
    "                ancestors = list(token.ancestors)\n",
    "                if (token.pos_ == \"VERB\" and len(ancestors) == 1\\\n",
    "                    and ancestors[0] == root_token):\n",
    "                    other_verbs.append(token)\n",
    "            return other_verbs\n",
    "        other_verbs = find_other_verbs(doc, root_token)\n",
    "        print(other_verbs)\n",
    "        def get_clause_token_span_for_verb(verb, doc, all_verbs):\n",
    "            if len(verb)!=0:\n",
    "                first_token_index = len(doc)\n",
    "                last_token_index = 0\n",
    "                this_verb_children = list(verb.children)\n",
    "                #\n",
    "                for child in this_verb_children:\n",
    "                    if (child not in all_verbs):\n",
    "                        if (child.i < first_token_index):\n",
    "                            first_token_index = child.i\n",
    "                        if (child.i > last_token_index):\n",
    "                            last_token_index = child.i\n",
    "                return(first_token_index, last_token_index)\n",
    "        \n",
    "        token_spans = []   \n",
    "        all_verbs = [root_token] + other_verbs\n",
    "        for other_verb in all_verbs:\n",
    "            (first_token_index, last_token_index) = \\\n",
    "             get_clause_token_span_for_verb(other_verb, \n",
    "                                            doc, all_verbs)\n",
    "            token_spans.append((first_token_index, \n",
    "                                last_token_index))\n",
    "        sentence_clauses = []\n",
    "        for token_span in token_spans:\n",
    "            start = token_span[0]\n",
    "            end = token_span[1]\n",
    "            if (start < end):\n",
    "                clause = doc[start:end]\n",
    "                sentence_clauses.append(clause)\n",
    "        sentence_clauses = sorted(sentence_clauses, \n",
    "                                  key=lambda tup: tup[0])\n",
    "        clauses_text = [clause.text for clause in sentence_clauses]\n",
    "        #print(clauses_text)\n",
    "        return len(clauses_text)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "8f69d640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "tt=check_clauses(doc)\n",
    "print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8725496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  num_clauses\n",
      "0  Drinks were bad, the hot chocolate was watered...          NaN\n",
      "1  This was the worst experience I've ever had a ...          NaN\n",
      "2  This is located on the site of the old Spruce ...          NaN\n",
      "3  I enjoyed coffee and breakfast twice at Toast ...          NaN\n",
      "4  I love Toast! The food choices are fantastic -...          NaN\n"
     ]
    }
   ],
   "source": [
    "length=len(review)\n",
    "df=review.copy()\n",
    "df['num_clauses']=np.nan\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56db9b5",
   "metadata": {},
   "source": [
    "### Test with first 5 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "c7d3b6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drinks were bad\n",
      "were\n",
      "1\n",
      " the hot chocolate was watered down and the latte had a burnt taste to it\n",
      "watered\n",
      "3\n",
      " the food was also poor quality\n",
      "was\n",
      "4\n",
      " but the service was the worst part\n",
      "was\n",
      "5\n",
      " their cashier was very rude\n",
      "was\n",
      "6\n",
      "\n",
      "6\n",
      "this was the worst experience i have ever had a casual coffee light fare place\n",
      "was\n",
      "1\n",
      " the server disappeared for  minutes\n",
      "disappeared\n",
      "2\n",
      " just talking to his friend by the window as my girlfriend and i sat dumbfounded that this dude had the nerve to do that on the job\n",
      "sat\n",
      "3\n",
      " we are trying to make eye contact\n",
      "trying\n",
      "5\n",
      " but clearly getting paid to talk to his bud was more important to him\n",
      "was\n",
      "6\n",
      " my girlfriend went up to the counter once the server disappeared into the back for another  minutes (what is this guy doing\n",
      "went\n",
      "9\n",
      ") and asked if she should order food up there or something\n",
      "asked\n",
      "10\n",
      " the girl at the counter gives her a weird look and just says \"i will get your server\n",
      "gives\n",
      "12\n",
      "\" when they arrive from the back\n",
      "arrive\n",
      "13\n",
      " they look over at our table and have a laugh\n",
      "look\n",
      "14\n",
      " yeah\n",
      "yeah\n",
      "14\n",
      " leaving us hanging for half-a-goddamn hour at a place with only two other customers is not funny - but in retrospect\n",
      "is\n",
      "16\n",
      " your collective incompetence and false sense of entitlement certainly was\n",
      "was\n",
      "17\n",
      " the food was okay\n",
      "was\n",
      "18\n",
      " for a place called \"toast\n",
      "for\n",
      "19\n",
      "\" i had figured the bread would be better\n",
      "be\n",
      "21\n",
      " but it was just cold le bus\n",
      "was\n",
      "22\n",
      " additionally\n",
      "additionally\n",
      "22\n",
      " i am % sure the \"andouille\" in my special was just a link of the pre-packaged offering from trader joe is cut into four pieces\n",
      "cut\n",
      "23\n",
      " $ for unapologetic mediocrity will not be happening again\n",
      "happening\n",
      "24\n",
      " avoid this place like the plague\n",
      "avoid\n",
      "25\n",
      " i almost did not leave a tip\n",
      "leave\n",
      "26\n",
      " and honestly i should not have\n",
      "have\n",
      "27\n",
      " i felt the buyer is remorse all day\n",
      "felt\n",
      "28\n",
      " what a disgrace\n",
      "disgrace\n",
      "29\n",
      "\n",
      "29\n",
      "this is located on the site of the old spruce street video\n",
      "located\n",
      "1\n",
      " the mild cofee is very good and the pastris are great\n",
      "is\n",
      "2\n",
      " at times\n",
      "at\n",
      "3\n",
      " the service is slow even when it is not busy and at other times some patrons receive a complimentary mimosa drink\n",
      "is\n",
      "5\n",
      " the wifi is good\n",
      "is\n",
      "6\n",
      "\n",
      "6\n",
      "i enjoyed coffee and breakfast twice at toast during my recent visit to philly\n",
      "enjoyed\n",
      "1\n",
      " the first morning i enjoyed the omelette du jour which had a savory filling of roast tomato\n",
      "enjoyed\n",
      "2\n",
      " portobello\n",
      "portobello\n",
      "2\n",
      " artichoke\n",
      "artichoke\n",
      "2\n",
      " goat cheese\n",
      "cheese\n",
      "2\n",
      " and wilted spinach\n",
      "wilted\n",
      "3\n",
      " it was accompanied by a crisp small side salad of baby greens\n",
      "accompanied\n",
      "4\n",
      " tomato\n",
      "tomato\n",
      "4\n",
      " and berries\n",
      "berries\n",
      "4\n",
      " the house dressing was light and complementary\n",
      "was\n",
      "5\n",
      " not at all over powering\n",
      "powering\n",
      "5\n",
      " i just had to complement the chef\n",
      "had\n",
      "7\n",
      " the barista recommended a pour over coffee and it did not disappoint\n",
      "recommended\n",
      "9\n",
      " he prepared my cup with care and attention to time and form\n",
      "prepared\n",
      "10\n",
      " i followed the pour over with a wonderful latte\n",
      "followed\n",
      "11\n",
      " again prepared very well\n",
      "prepared\n",
      "12\n",
      " as i was leaving town\n",
      "leaving\n",
      "13\n",
      " i decided to visit again for breakfast before my long trip ahead\n",
      "decided\n",
      "14\n",
      " this breakfast was a delicious eggs benedict combo that was perfectly done and complemented by mushrooms\n",
      "was\n",
      "15\n",
      " roast tomato\n",
      "tomato\n",
      "15\n",
      " and a generous helping of wilted greens\n",
      "helping\n",
      "16\n",
      " a rich cup of melted butter accompanied the dish and the sweet side salad again brought balance to my meal\n",
      "accompanied\n",
      "18\n",
      " this time the barista made me a perfect cup of french pressed guatemalan vienna roast coffee\n",
      "made\n",
      "19\n",
      " it was balanced and bright\n",
      "was\n",
      "20\n",
      " yet rich in flavor\n",
      "rich\n",
      "21\n",
      " i had to have a second cup to go\n",
      "had\n",
      "22\n",
      " cudos to the barista\n",
      "cudos\n",
      "23\n",
      " chef\n",
      "chef\n",
      "23\n",
      " and friendly staff\n",
      "staff\n",
      "24\n",
      " toast is definitely a place to visit\n",
      "is\n",
      "25\n",
      " eat\n",
      "eat\n",
      "25\n",
      " and enjoy coffee prepared in a variety ways\n",
      "enjoy\n",
      "26\n",
      " do not miss this one\n",
      "miss\n",
      "27\n",
      "\n",
      "27\n",
      "i love toast\n",
      "love\n",
      "1\n",
      " the food choices are fantastic - i love that they serve brunch all day\n",
      "love\n",
      "3\n",
      " and their coffee is well brewed and prepared\n",
      "is\n",
      "5\n",
      " i am a fan of the large windows - it is the perfect location to sit and people watch for a little while\n",
      "is\n",
      "7\n",
      " nestled in center city\n",
      "nestled\n",
      "8\n",
      " toast provides people like me who travel into the city for work a little haven to de-stress and kick back a little bit\n",
      "provides\n",
      "9\n",
      " the staff is wonderfully friendly and always eager to provide their own suggestions when you are not sure what to get\n",
      "is\n",
      "10\n",
      " now that school is starting back up it also makes it the perfect environment to settle down and do some studying\n",
      "makes\n",
      "13\n",
      " i love the fact that they play maps by the yeah yeah yeah is - and related pandora stations - it really sets the mood to unwind or have casual conversation with some friends\n",
      "sets\n",
      "14\n",
      " can not wait to go back\n",
      "wait\n",
      "16\n",
      "\n",
      "16\n",
      "                                              review  num_clauses\n",
      "0  Drinks were bad, the hot chocolate was watered...          6.0\n",
      "1  This was the worst experience I've ever had a ...         29.0\n",
      "2  This is located on the site of the old Spruce ...          6.0\n",
      "3  I enjoyed coffee and breakfast twice at Toast ...         27.0\n",
      "4  I love Toast! The food choices are fantastic -...         16.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    text=review.iloc[i,0].lower()\n",
    "    text = text.replace(\"_\", \" \")\n",
    "    text = text.replace(\"/\", \" \")\n",
    "    text = text.replace(\"\\\\\", \" \")\n",
    "    #text = text.replace(\"\\\\x\", \" \")   # deal with \\x..........\n",
    "    text = re.sub(br'(\\xc2)(.)', b'', text.encode('utf-8')).decode()\n",
    "    text = text.replace(\"can't\", \"can not\")\n",
    "    text = text.replace(\"won't\", \"will not\")\n",
    "    text = text.replace(\"'ve\",\" have\")\n",
    "    text = text.replace(\"'d\",\" had\")\n",
    "    text = text.replace(\"'m\", \" am\")\n",
    "    text = text.replace(\"'ll\", \" will\")\n",
    "    text = text.replace(\"'s\", \" is\")\n",
    "    text = text.replace(\"n't\", \" not\")\n",
    "    text = text.replace(\"'re\", \" are\")\n",
    "    text = text.replace(\"st.\", \"street\")\n",
    "    text = text.replace(\"bldg.\", \"building\") \n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text=re.split('[,.?!]',text)\n",
    "    count=0\n",
    "    for phrase in text:\n",
    "        print(phrase)\n",
    "        t=nlp(phrase)\n",
    "        count+=check_clauses(t)\n",
    "        print(count)\n",
    "    df.iloc[i,1]=count\n",
    "print(df.head())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7143409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_clauses(doc):\n",
    "    #print('doc',doc)\n",
    "    if len(doc)!=0 and doc!=None:\n",
    "        def find_root_of_sentence(doc):\n",
    "            root_token = None\n",
    "            for token in doc:\n",
    "                if (token.dep_ == \"ROOT\"):\n",
    "                    root_token = token\n",
    "            return root_token\n",
    "        root_token = find_root_of_sentence(doc)\n",
    "        if root_token!=None:\n",
    "            #print('root token', root_token)\n",
    "            def find_other_verbs(doc, root_token):\n",
    "                other_verbs = []\n",
    "                for token in doc:\n",
    "                    ancestors = list(token.ancestors)\n",
    "                    if (token.pos_ == \"VERB\" and len(ancestors) == 1\\\n",
    "                        and ancestors[0] == root_token):\n",
    "                        other_verbs.append(token)\n",
    "                return other_verbs\n",
    "            other_verbs = find_other_verbs(doc, root_token)\n",
    "            #print('other verbs',other_verbs)\n",
    "            def get_clause_token_span_for_verb(verb, doc, all_verbs):\n",
    "                if len(verb)!=0:\n",
    "                    first_token_index = len(doc)\n",
    "                    last_token_index = 0\n",
    "                    this_verb_children = list(verb.children)\n",
    "                    #\n",
    "                    for child in this_verb_children:\n",
    "                        if (child not in all_verbs):\n",
    "                            if (child.i < first_token_index):\n",
    "                                first_token_index = child.i\n",
    "                            if (child.i > last_token_index):\n",
    "                                last_token_index = child.i\n",
    "                    return(first_token_index, last_token_index)\n",
    "\n",
    "            token_spans = []   \n",
    "            all_verbs = [root_token] + other_verbs\n",
    "            for other_verb in all_verbs:\n",
    "                (first_token_index, last_token_index) = \\\n",
    "                 get_clause_token_span_for_verb(other_verb, \n",
    "                                                doc, all_verbs)\n",
    "                token_spans.append((first_token_index, \n",
    "                                    last_token_index))\n",
    "            sentence_clauses = []\n",
    "            for token_span in token_spans:\n",
    "                start = token_span[0]\n",
    "                end = token_span[1]\n",
    "                if (start < end):\n",
    "                    clause = doc[start:end]\n",
    "                    sentence_clauses.append(clause)\n",
    "            sentence_clauses = sorted(sentence_clauses, \n",
    "                                      key=lambda tup: tup[0])\n",
    "            clauses_text = [clause.text for clause in sentence_clauses]\n",
    "            #print(clauses_text)\n",
    "            return len(clauses_text)\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c49b712",
   "metadata": {},
   "source": [
    "### Implement to whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f860c245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  num_clauses\n",
      "0  Drinks were bad, the hot chocolate was watered...          NaN\n",
      "1  This was the worst experience I've ever had a ...          NaN\n",
      "2  This is located on the site of the old Spruce ...          NaN\n",
      "3  I enjoyed coffee and breakfast twice at Toast ...          NaN\n",
      "4  I love Toast! The food choices are fantastic -...          NaN\n"
     ]
    }
   ],
   "source": [
    "length=len(review)\n",
    "df=review.copy()\n",
    "df['num_clauses']=np.nan\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05160213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "139000\n",
      "140000\n",
      "141000\n",
      "142000\n",
      "143000\n",
      "144000\n",
      "145000\n",
      "146000\n",
      "147000\n",
      "148000\n",
      "149000\n",
      "150000\n",
      "151000\n",
      "152000\n",
      "153000\n",
      "154000\n",
      "155000\n",
      "156000\n",
      "157000\n",
      "158000\n",
      "159000\n",
      "160000\n",
      "161000\n",
      "162000\n",
      "163000\n",
      "164000\n",
      "165000\n",
      "166000\n",
      "167000\n",
      "168000\n",
      "169000\n",
      "170000\n",
      "171000\n",
      "172000\n",
      "173000\n",
      "174000\n",
      "175000\n",
      "176000\n",
      "177000\n",
      "178000\n",
      "179000\n",
      "180000\n",
      "181000\n",
      "182000\n",
      "183000\n",
      "184000\n",
      "185000\n",
      "186000\n",
      "187000\n",
      "188000\n",
      "189000\n",
      "190000\n",
      "191000\n",
      "192000\n",
      "193000\n",
      "194000\n",
      "195000\n",
      "196000\n",
      "197000\n",
      "198000\n",
      "199000\n",
      "200000\n",
      "201000\n",
      "202000\n",
      "203000\n",
      "204000\n",
      "205000\n",
      "206000\n",
      "207000\n",
      "208000\n",
      "209000\n",
      "210000\n",
      "211000\n",
      "212000\n",
      "213000\n",
      "214000\n",
      "215000\n",
      "216000\n",
      "217000\n",
      "218000\n",
      "219000\n",
      "220000\n",
      "221000\n",
      "222000\n",
      "223000\n",
      "224000\n",
      "225000\n",
      "226000\n",
      "227000\n",
      "228000\n",
      "229000\n",
      "230000\n",
      "231000\n",
      "232000\n",
      "233000\n",
      "234000\n",
      "235000\n",
      "236000\n",
      "237000\n",
      "238000\n",
      "239000\n",
      "240000\n",
      "241000\n",
      "242000\n",
      "243000\n",
      "244000\n",
      "245000\n",
      "246000\n",
      "247000\n",
      "248000\n",
      "249000\n",
      "250000\n",
      "251000\n",
      "252000\n",
      "253000\n",
      "254000\n",
      "255000\n",
      "256000\n",
      "257000\n",
      "258000\n",
      "259000\n",
      "260000\n",
      "261000\n",
      "262000\n",
      "263000\n",
      "264000\n",
      "265000\n",
      "266000\n",
      "267000\n",
      "268000\n",
      "269000\n",
      "270000\n",
      "271000\n",
      "272000\n",
      "273000\n",
      "274000\n",
      "275000\n",
      "276000\n",
      "277000\n",
      "278000\n",
      "279000\n",
      "280000\n",
      "281000\n",
      "282000\n",
      "283000\n",
      "284000\n",
      "285000\n",
      "286000\n",
      "287000\n",
      "288000\n",
      "289000\n",
      "290000\n",
      "291000\n",
      "292000\n",
      "293000\n",
      "294000\n",
      "295000\n",
      "296000\n",
      "297000\n",
      "298000\n",
      "299000\n",
      "300000\n",
      "301000\n",
      "302000\n",
      "303000\n",
      "304000\n",
      "305000\n",
      "306000\n",
      "307000\n",
      "308000\n",
      "309000\n",
      "310000\n",
      "311000\n",
      "312000\n",
      "313000\n",
      "314000\n",
      "315000\n",
      "316000\n",
      "317000\n",
      "318000\n",
      "319000\n",
      "320000\n",
      "321000\n",
      "322000\n",
      "323000\n",
      "324000\n",
      "325000\n",
      "326000\n",
      "327000\n",
      "328000\n",
      "329000\n",
      "330000\n",
      "331000\n",
      "332000\n",
      "333000\n",
      "334000\n",
      "335000\n",
      "336000\n",
      "337000\n",
      "338000\n",
      "339000\n",
      "340000\n",
      "341000\n",
      "342000\n",
      "343000\n",
      "344000\n",
      "345000\n",
      "346000\n",
      "347000\n",
      "348000\n",
      "349000\n",
      "350000\n",
      "351000\n",
      "352000\n",
      "353000\n",
      "354000\n",
      "355000\n",
      "356000\n",
      "357000\n",
      "358000\n",
      "359000\n",
      "360000\n",
      "361000\n",
      "362000\n",
      "363000\n",
      "364000\n",
      "365000\n",
      "366000\n",
      "367000\n",
      "368000\n",
      "369000\n",
      "370000\n",
      "371000\n",
      "372000\n",
      "373000\n",
      "374000\n",
      "375000\n",
      "376000\n",
      "377000\n",
      "378000\n",
      "379000\n",
      "380000\n",
      "381000\n",
      "382000\n",
      "383000\n",
      "384000\n",
      "385000\n",
      "386000\n",
      "387000\n",
      "388000\n",
      "389000\n",
      "390000\n",
      "391000\n",
      "392000\n",
      "393000\n",
      "394000\n",
      "395000\n",
      "396000\n",
      "397000\n",
      "398000\n",
      "399000\n",
      "400000\n",
      "401000\n",
      "402000\n",
      "403000\n",
      "404000\n",
      "405000\n",
      "406000\n",
      "407000\n",
      "408000\n",
      "409000\n",
      "410000\n",
      "411000\n",
      "412000\n",
      "413000\n",
      "414000\n",
      "415000\n",
      "416000\n",
      "417000\n",
      "418000\n",
      "419000\n",
      "420000\n",
      "421000\n",
      "422000\n",
      "423000\n",
      "424000\n",
      "425000\n",
      "426000\n",
      "427000\n",
      "428000\n",
      "429000\n",
      "430000\n",
      "431000\n",
      "432000\n",
      "433000\n",
      "434000\n",
      "435000\n",
      "436000\n",
      "437000\n",
      "438000\n",
      "439000\n",
      "440000\n",
      "441000\n",
      "442000\n",
      "443000\n",
      "444000\n",
      "445000\n",
      "446000\n",
      "447000\n",
      "448000\n",
      "449000\n",
      "450000\n",
      "451000\n",
      "452000\n",
      "453000\n",
      "454000\n",
      "455000\n",
      "456000\n",
      "457000\n",
      "458000\n",
      "459000\n",
      "460000\n",
      "461000\n",
      "462000\n",
      "463000\n",
      "464000\n",
      "465000\n",
      "466000\n",
      "467000\n",
      "468000\n",
      "469000\n",
      "470000\n",
      "471000\n",
      "472000\n",
      "473000\n",
      "474000\n",
      "475000\n",
      "476000\n",
      "477000\n",
      "478000\n",
      "479000\n",
      "480000\n",
      "481000\n",
      "482000\n",
      "483000\n",
      "484000\n",
      "485000\n",
      "486000\n",
      "487000\n",
      "488000\n",
      "489000\n",
      "490000\n",
      "491000\n",
      "492000\n",
      "493000\n",
      "494000\n",
      "495000\n",
      "496000\n",
      "497000\n",
      "498000\n",
      "499000\n",
      "500000\n",
      "501000\n",
      "502000\n",
      "503000\n",
      "504000\n",
      "505000\n",
      "506000\n",
      "507000\n",
      "508000\n",
      "509000\n",
      "510000\n",
      "511000\n",
      "512000\n",
      "513000\n",
      "514000\n",
      "515000\n",
      "516000\n",
      "517000\n",
      "518000\n",
      "519000\n",
      "520000\n",
      "521000\n",
      "522000\n",
      "523000\n",
      "524000\n",
      "525000\n",
      "526000\n",
      "527000\n",
      "528000\n",
      "529000\n",
      "530000\n",
      "531000\n",
      "532000\n",
      "533000\n",
      "534000\n",
      "535000\n",
      "536000\n",
      "537000\n",
      "538000\n",
      "539000\n",
      "540000\n",
      "541000\n",
      "542000\n",
      "543000\n",
      "544000\n",
      "545000\n",
      "546000\n",
      "547000\n",
      "548000\n",
      "549000\n",
      "550000\n",
      "551000\n",
      "552000\n",
      "553000\n",
      "554000\n",
      "555000\n",
      "556000\n",
      "557000\n",
      "558000\n",
      "559000\n",
      "560000\n",
      "561000\n",
      "562000\n",
      "563000\n",
      "564000\n",
      "565000\n",
      "566000\n",
      "567000\n",
      "568000\n",
      "569000\n",
      "570000\n",
      "571000\n",
      "572000\n",
      "573000\n",
      "574000\n",
      "575000\n",
      "576000\n",
      "577000\n",
      "578000\n",
      "579000\n",
      "580000\n",
      "581000\n",
      "582000\n",
      "583000\n",
      "584000\n",
      "585000\n",
      "586000\n",
      "587000\n",
      "588000\n",
      "589000\n",
      "590000\n",
      "591000\n",
      "592000\n",
      "593000\n",
      "594000\n",
      "595000\n",
      "596000\n",
      "597000\n",
      "598000\n",
      "599000\n",
      "600000\n",
      "601000\n",
      "602000\n",
      "603000\n",
      "604000\n",
      "605000\n",
      "606000\n",
      "607000\n",
      "608000\n",
      "CPU times: total: 12h 11min 59s\n",
      "Wall time: 12h 14min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(length):\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "    text=review.iloc[i,0].lower()\n",
    "    text = text.replace(\"_\", \" \")\n",
    "    text = text.replace(\"/\", \" \")\n",
    "    text = text.replace(\"\\\\\", \" \")\n",
    "    #text = text.replace(\"\\\\x\", \" \")   # deal with \\x..........\n",
    "    text = re.sub(br'(\\xc2)(.)', b'', text.encode('utf-8')).decode()\n",
    "    text = text.replace(\"can't\", \"can not\")\n",
    "    text = text.replace(\"won't\", \"will not\")\n",
    "    text = text.replace(\"'ve\",\" have\")\n",
    "    text = text.replace(\"'d\",\" had\")\n",
    "    text = text.replace(\"'m\", \" am\")\n",
    "    text = text.replace(\"'ll\", \" will\")\n",
    "    text = text.replace(\"'s\", \" is\")\n",
    "    text = text.replace(\"n't\", \" not\")\n",
    "    text = text.replace(\"'re\", \" are\")\n",
    "    text = text.replace(\"st.\", \"street\")\n",
    "    text = text.replace(\"bldg.\", \"building\") \n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text=re.split('[,.?!]',text)\n",
    "    count=0\n",
    "    for phrase in text:\n",
    "        t=nlp(phrase)\n",
    "        count+=check_clauses(t)\n",
    "    df.iloc[i,1]=count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46f089be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  num_clauses\n",
      "0  Drinks were bad, the hot chocolate was watered...          6.0\n",
      "1  This was the worst experience I've ever had a ...         29.0\n",
      "2  This is located on the site of the old Spruce ...          6.0\n",
      "3  I enjoyed coffee and breakfast twice at Toast ...         27.0\n",
      "4  I love Toast! The food choices are fantastic -...         16.0\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ea9253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import savetxt\n",
    "savetxt(\"feature_num_clauses.txt\",df,fmt=\"%s\",delimiter=\"\\t\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a239b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
